\relax 
\citation{sutton_barto_12}
\@writefile{toc}{\contentsline {section}{\numberline {I}Einf\IeC {\"u}hrung}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-A}}Subsection Heading Here}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {I-A}1}Subsubsection Heading Here}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Ergebnis}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Reinforcement Learning (RL)}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Markov Decision Process}{1}}
\citation{sutton_barto_12}
\citation{goodfellow_16}
\bibcite{goodfellow_16}{1}
\bibcite{sutton_barto_12}{2}
\bibcite{sutton_99}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Die Interaktion zwischen Agent und Umgebung nach \cite  {sutton_barto_12}.}}{2}}
\newlabel{agent_environment}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Die Strategie (Policy $\pi $)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Die Bewertungsfunktion (Value Function)}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Value-Based Reinforcement Learning}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Policy-Based Reinforcement Learning}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Funktionsapproximation}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Policy Gradient Methoden}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VII-A}}Finite Difference Policy Gradient}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VII-B}}Monte-Carlo Policy Gradient (REINFORCE)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VII-C}}Actor-Critic Policy Gradient}{2}}
\@writefile{toc}{\contentsline {section}{References}{2}}

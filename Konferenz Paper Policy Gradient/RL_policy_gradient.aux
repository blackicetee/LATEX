\relax 
\citation{sutton_barto_12}
\citation{sutton_barto_12}
\citation{sutton_barto_12}
\@writefile{toc}{\contentsline {section}{\numberline {I}Einf\IeC {\"u}hrung}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Reinforcement Learning (RL)}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Markov Decision Process}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Die Strategie (Policy $\pi $)}{1}}
\citation{hoever_14}
\citation{sutton_99}
\citation{silver_15}
\bibcite{sutton_barto_12}{1}
\bibcite{sutton_99}{2}
\bibcite{hoever_14}{3}
\bibcite{silver_15}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Die Interaktion zwischen Agent und Umgebung nach \cite  {sutton_barto_12}.}}{2}}
\newlabel{agent_environment}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Die Bewertungsfunktion (Value Function)}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Gradientenbasierte Optimierung}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Zielfunktionen}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Policy Gradient}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Funktionsapproximation}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Policy Gradient Methoden}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VII-A}}Finite Difference Policy Gradient}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VII-B}}Monte-Carlo Policy Gradient (REINFORCE)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VII-C}}Actor-Critic Policy Gradient}{2}}
\@writefile{toc}{\contentsline {section}{References}{2}}
\bibcite{goodfellow_16}{5}

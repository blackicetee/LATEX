\chapter{Auswertung}
\label{cha:Auswertung}
In diesem Kapitel wollen wir die Testergebnisse des vorherigen Kapitels \ref{cha:Validierung} auswerten. Die Ergebnisse der Testphase sollen uns dabei helfen die Leistungsfähigkeit und die Grenzen des TD-Q-Lernens zu beurteilen. Wir werden ein, in der Literatur bereits bekanntes Problem dieses Lernverfahrens mit den Testergebnissen belegen und Lösungsmöglichkeiten besprechen. Diese Lösungsmöglichkeiten sind aus der Literatur entnommen und sie stellen herausragende Erfolge auf dem Gebiet des verstärkenden Lernens dar. \\

\section{TD-Q-Lernen - Leistung und Grenzen}
\label{sec:TD-Q-Lernen - Leistung und Grenzen}
Die Testergebnisse ermöglichen uns, über die Konvergenz und über die benötigte Rechenzeit der gelernten Strategien, in den verschiedenen Testphasen, Aussagen zu treffen. Die Konvergenz bezieht sich auf eine Annäherung der vom TD-Q-Agenten gelernten Strategie an eine unbekannte optimale Strategie. \\

Wolfgang Ertel schreibt über die allgemeine Konvergenz des Q-Lernens (TD-Q-Lernens) (vgl. \cite[299]{Ertel}): \\

Das Q-Lernen konvergiert für ein konkretes Beispiel und allgemein, zu einer optimalen Strategie, wenn jedes Zustands-Aktions-Paar unendlich oft besucht wird. Konkret konvergiert der Wert $\hat{Q}_n(s,a)$ für alle Werte von s und a gegen $Q(s,a)$ für $n \rightarrow \infty$, mit $n$ gleich der Anzahl der Aktualisierungen des Q-Werts ($\hat{Q}_n(s,a)$).

\subsection{TD-Q-Lernen Leistungsfähigkeit (Konvergenz)}
In diesem Abschnitt analysieren wir die Testergebnisse der einzelnen Lernphasen, für 9 und 16 Spielfelder Tic Tac Toe. Wir beurteilen die Leistung der einzelnen gelernten Strategien, aufgrund der Testspiele gegen den Zufallsagenten und den Heuristik Agenten. Wir versuchen, die oben zitierte Aussage von Wolfgang Ertel, über die Konvergenz, mit unseren Testergebnissen zu untermauern.  \\

\myparagraph{Tic Tac Toe - Lernen gegen Zufall - 9 Spielfelder}

Zufallsagent beginnt das Spiel: \\
Die in 100 Trainingsspielen gelernte Strategie des TD-Q-Agenten hat schlechtere Gewinnquoten (37 Siege), als die in 1.000 Trainingsspielen gelernte Strategie. Die in 10.000 Trainingsspielen gelernte Strategie, hat die besten Gewinnquoten (49 Siege). Die Steigerung der Gewinnquoten der einzelnen Lernphasen ist hier noch recht klein. \\

TD-Q-Agent beginnt das Spiel: \\
Die in 100 Trainingsspielen gelernte Strategie des TD-Q-Agenten hat schlechtere Gewinnquoten (73 Siege), als die in 1.000 Trainingsspielen gelernte Strategie. Die in 10.000 Trainingsspielen gelernte Strategie, hat die besten Gewinnquoten (92 Siege).  

Wir können aus den Testergebnissen ableiten, dass die Leistungsfähigkeit ansteigt, je mehr Trainingsspiele der TD-Q-Agent durchführen konnte. Die Testergebnisse der Lernphasen zeigen sehr gut die Auswirkungen, die die Anzahl der Trainingsspiele auf die Konvergenz der gelernten Strategie hat. \\

Obwohl es praktisch nicht möglich ist, unendlich viele Trainingsspiele durchzuführen, konnten wir zeigen, dass die gelernten Strategien eher zu einer optimalen Strategie konvergieren, je mehr Trainingsspiele durchgeführt werden. Die oben zitierte Aussage von Wolfgang Ertel, über die Konvergenz, trifft für diese Testergebnisse demnach annähern zu. \\

\myparagraph{Tic Tac Toe - Lernen gegen Zufall - 16 Spielfelder}

Beim 16 Spielfelder Tic Tac Toe sind die Aktions- und Zustandsräume wesentlich umfangreicher, als beim 9 Spielfelder Tic Tac Toe. Den Testergebnissen können wir entnehmen, dass beim 16 Spielfelder Tic Tac TOe keine eindeutige Steigerung der Gewinnquote mit ansteigender Trainingsspielanzahl erkennbar ist, egal welcher der beiden Agenten das Spiel beginnt. Auf Grund der erhöhten Komplexität, sind mit hoher Wahrscheinlichkeit wesentlich mehr Trainingsspiele erforderlich, um auch hier eine annähernde Konvergenz der gelernten Strategie zu erreichen.

In Abschnitt \ref{subsec:TD-Q-Lernen Grenzen} klären wir noch ausführlich, warum die Testergebnisse der einzelnen Lernphasen, für das 16 Spielfelder Tic Tac Toe, keine eindeutige Leistungssteigerung aufzeigen, obwohl die Trainingsspiele in jeder Lernphase um Faktor 10 erhöht wurden. \\

\myparagraph{Tic Tac Toe - Lernen gegen Heuristik - 9 und 16 Spielfelder}

Die Testspiele der gelernten TD-Q-Strategien gegen den vorausschauenden Heuristik Agent sind insofern eindeutig ausgefallen, dass der Heuristik Agent bis auf 2 gelernte Strategien, immer die bessere Strategie hatte und gewinnt. In zwei Lernphasen geht das Spiel unentschieden aus. \\

In jeder der 6 Lernphasen (3 Lernphasen 9 Spielfelder und 3 Lernphasen 16 Spielfelder) wird eine unterschiedliche Strategie gelernt. In den Lernphasen wurden 6 verschiedene Strategien gelernt. \\

In den einzelnen Testphasen haben die beiden Agenten, je nachdem wer beginnt, in den jeweils 100 Testspielen das gleiche Spiel gespielt. Der Grund dafür ist, nach Abschluss der Lernphasen entstehen feste nicht veränderbare Strategien. Diese entscheiden für den selben Ausgangszustand immer mit der selben Aktion, das Gleiche gilt auch für den Heuristik Agenten. Insgesamt wurden in den 1.200 Testspielen nur 12 unterschiedliche Spiele gespielt. Eine Erkenntnis der Auswertung der Testergebnisse ist deshalb auch, dass es  ausreichend gewesen wäre, wenn in jeder Testphase maximal 2 Testspiele durchgeführt worden wären. Einmal hätte der Heuristik Agent das Spiel begonnen und einmal der TD-Q-Agent. \\ 

Folglich ist die 2. Hypothese aus dem Abschnitt \ref{sec:Ergebnisse} bestätigt. \\

\myparagraph{Tic Tac Toe - Heuristik gegen Zufall - 9 und 16 Spielfelder}

Die Testergebnisse des vorausschauenden Heuristik Agenten gegen den Zufallsagenten sind eindeutig. Der kleinste Erfolg des Heuristik Agenten, mit 64\% Gewinnquote bestätigt bereits die 1. Hypothese aus Abschnitt \ref{sec:Ergebnisse}. Die größte Gewinnquote des von uns implementierten Tic Tac Toe vorausschauenden Heuristik Agenten ist 100\%, d.h. von 100 Testspielen in einem 16 Spielfelder Tic Tac Toe, wenn der Heuristik Agent das Spiel beginnt, gewinnt der Heuristik Agent alle 100 Testspiele. \\

\subsection{TD-Q-Lernen Grenzen (Fluch der Dimensionalität)}
\label{subsec:TD-Q-Lernen Grenzen}
''Trotz der Erfolge in den letzten Jahren bleibt das Lernen durch Verstärkung ein sehr attraktives Forschungsgebiet der KI, nicht zuletzt deshalb, weil auch die besten heute bekannten Lernalgorithmen bei hochdimensionalen Zustands- und Aktionsräumen wegen ihrer gigantischen Rechenzeit immer noch nicht praktisch anwendbar sind. \cite[305]{Ertel}''\\

Wolfgang Ertel beschreibt in diesem Zitat den Fluch der Dimensionalität, als großes Problem der heutigen Lernalgorithmen. Auch Russell und Norvig erwähnen dieses Problem:\\

''Bisher sind wir davon ausgegangen, dass die Nutzenfunktionen und die von den Agenten gelernten Q-Funktionen in tabellarischer Form mit einem Ausgabewert für jedes Eingabetupel vorliegen. Ein solcher Ansatz funktioniert ausreichend gut für kleine Zustandsräume, aber die Zeit bis zur Konvergenz und (für ADP) die Zeit pro Iteration steigt mit wachsendem Raum rapide an. \cite[975]{Russell}''\\

Stellen wir uns einen Suchbaum vor (ähnlich der Abbildung \ref{fig:minimax_tictactoe} aus dem Abschnitt Minimax-Suche), der alle Aktionen in jedem Zustand eines 9 Spielfelder Tic Tac Toe Spiels abbildet. Das Spielbrett eines 9 Spielfelder Tic Tac Toe's ist 3 mal 3 Spielfelder groß, die Dimension des 9 Spielfelder Tic Tac Toe's ist also gleich 3. Der Verzweigungsfaktor des Suchbaums ist direkt nach dem Wurzelknoten gleich 9. Erhöhen wir die Tiefe des Suchbaums um 1, dann verringert sich der Verzweigungsfaktor ebenfalls um 1 (gilt für Reversi und Tic Tac Toe). Der Verzweigungsfaktor symbolisiert den Aktionsraum, der in einem bestimmten Spielzustand vorhanden ist. Jeder Knoten (auch Blattknoten und Wurzelknoten) ist ein möglicher Spielzustand. Die Gesamtheit aller Spielzustände bildet den Zustandsraum ab. \\

Wenn wir die redundanten Spielzustände nicht ausschließen und den Fakt nicht beachten, dass nicht jeder Baumpfad eine maximale Länge von 9 hat (9 Spielfelder Tic Tac Toe kann schon in einer Tiefe von 5 terminieren), dann hat der Spielbaum geschätzt 362.880 Spielzustände ($9 * 8 * 7 * 6 * 5 * 4 * 3 * 2$ entspricht 9 Fakultät!). Erhöhen wir die Dimension des Tic Tac Toe Spiels auf 4, dann erhalten wir das 16 Spielfelder Tic Tac Toe. Berechnen wir die Spielzustände für das 16 Spielfelder Tic Tac Toe unter den gleichen Voraussetzungen wie bei dem 9 Spielfelder Tic Tac Toe, dann erhalten wir ungefähr 20.922.789.888.000 Spielzustände. Die Anzahl der Berechneten Spielzustände entspricht einer groben Schätzung und soll zur Veranschaulichung der Dimensionen dienen.

Diese Anzahl der Spielzustände entsprechen nicht der tatsächlichen Anzahl, sie sollen hier nur als grobe Schätzung und zur Veranschaulichung dienen (keine Garantie der Korrektheit der Werte). \\

Wir können aus diesen beiden Spielzustandsschätzungen schlussfolgern, dass eine Erhöhung der Dimension einen enormen Effekt auf die Vergrößerung des Zustands- und Aktionsraums hat. Vermutlich meint Wolfgang Ertel genau dieses Verhältnis, wenn er von Fluch der Dimensionalität spricht. \\

\myparagraph{Rechenzeit der Lernphasen}
Wir können die in der Testphase ermittelten Rechenzeitwerte der einzelnen Lernphasen verwenden, um den Fluch der Dimensionalität aufzuzeigen. \\

Rechenzeit des TD-Q-Lernens für 9 Spielfelder Tic Tac Toe: 
\begin{itemize}
\item 100 Trainingsspiele ungefähr 5 Minuten 
\item 1.000 Trainingsspiele ungefähr 25 Minuten
\item 10.000 Trainingsspiele ungefähr 180 Minuten
\end{itemize}

Rechenzeit des TD-Q-Lernens für 16 Spielfelder Tic Tac Toe: 
\begin{itemize}
\item 100 Trainingsspiele ungefähr 25 Minuten
\item 1.000 Trainingssiele ungefähr 180 Minuten
\item 10.000 Trainingsspiele ungefähr 1440 Minuten
\end{itemize}

Die einzelnen Testergebnisse zeigen deutlich eine Rechenzeiterhöhung der Lernphasen des 16 Spielfelder Tic Tac Toe's im Gegensatz zum 9 Spielfelder Tic Tac Toe, d.h. mit steigender Dimension erhöht sich die Rechenzeit für das TD-Q-Lernen in unserem Beispiel bereits um 800\%. Wir schlussfolgern daher: 10.000 Trainingsspiele für das Strategiespiel Reversi, würden mehrere Monate oder Jahre in Anspruch nehmen, bei Verwendung einer tabellarischen Darstellung der Q-Funktion. \\

Die bisherigen Ergebnisse dieser Arbeit bestätigen somit die nachfolgende Aussagen von Wolfgang Ertel: \\

''Die weltbesten Schachcomputer arbeiten bis heute immer noch ohne Lernverfahren. Dafür gibt es zwei Gründe. Einerseits benötigen die bis heute entwickelten Verfahren zum Lernen durch Verstärkung bei großen Zustandsräumen noch sehr viel Rechenzeit. Andererseits sind aber die manuell erstellten Heuristiken der Hochleistungsschachcomputer schon sehr stark optimiert. Das heißt, dass nur ein sehr gutes Lernverfahren noch zu Verbesserungen führen kann. \cite[120]{Ertel}''\\

\section{Lösungen für das Dimensionalitätsproblem}
\label{sec:Ausblick}
Im letzten Abschnitt dieser Arbeit werden wir auf die sehr guten Lernverfahren eingehen, die Wolfgang Ertel in seiner Aussage zuvor erwähnt hat. Die beiden vielleicht bisher erfolgreichsten lernenden Programme. Das Dame-Spiel von Arthur L. Samuel aus dem Jahre 1955 und TD-Gammon von Gerald Tesauro aus dem Jahre 1992. 

\subsection{Samuels-Dame-Spiel}
''Arthur L. Samuel schrieb 1955 ein Programm, dass Dame spielen konnte und mit einem einfachen Lernverfahren seine Parameter verbessern konnte. Sein Programm hatte dabei jedoch Zugriff auf eine große Zahl von archivierten Spielen, bei denen jeder einzelne Zug von Experten bewertet war (Überwachtes Lernen zur Unterstützung des verstärkenden Lernens). Damit verbesserte das Programm seine Bewertungsfunktion. Um eine noch weitere Verbesserung zu erreichen, ließ Samuel sein Programm gegen sich selbst spielen. Das Credit Assignment löste er auf einfache Weise. Für jede einzelne Stellung während eines Spiels vergleicht er die Bewertung durch die Funktion B(s) mit der durch Alpha-Beta-Pruning berechneten Bewertung und verändert B(s) entsprechend. 1961 besiegte sein Dame-Programm den viertbesten Damespieler der USA. Mit dieser bahnbrechenden Arbeit war Samuel seiner Zeit um fast dreißig Jahre voraus. \cite[120\psq]{Ertel}''\\

Wir werden nachfolgend den Unterschied zwischen dem in dieser Arbeit implementierten Lernverfahren und dem von Samuel implementierten Lernverfahren erläutern (vgl. \cite[976]{Russell}): \\

Russell und Norvig beschreiben das Lernverfahren von Samuel als \textbf{Funktionsannäherung}. Der Unterschied zwischen Samuels Lernalgorithmus und dem von uns implementierten TD-Q-Algorithmus ist, dass Samuel eine andere Darstellung als eine Suchtabelle (die Tabelle die wir mit Q(s,a) gekennzeichnet hatten) für die Q-Funktion verwendet. Arthur Samuels Dame Spiel verwendete also eine parametrisierte Bewertungsfunktion in der Form: \\
\begin{equation*}
\hat{U}_\theta(s) = \theta_1 f_1(s) + \theta_2 f_2(s) + ... + \theta_n f_n(s),
\end{equation*}

um die Q-Funktion darzustellen. Samuel verwendet für das Lernen der einzelnen Parameter ($\theta = \theta_1, ... \theta_n$) der Bewertungsfunktion $\hat{U}_\theta(s)$ eine abgewandelte Form, des in dieser Arbeit implementierten TD-Q-Lernens (vgl. \cite[981]{Russell}). Dieses Vorgehen ermöglichte es Samuels Lernalgorithmus die Q-Funktion, statt in einer $10^{40}$ Tabelle, durch etwa $n = 20$ Parameter zu charakterisieren. Dies ist gegenüber der Tabelle (unsere Darstellung der Q-Funktion) die unser TD-Q Agent in dieser Arbeit lernt, eine riesige Komprimierung. \\

\subsection{TD-Gammon}
\label{subsec:TD-Gammon}
''Das TD-Lernen zusammen mit einem Backpropagation-Netz mit 40 bis 80 verdeckten Neuronen wurde sehr erfolgreich angewendet in TD-Gammon, einem Programm zum Spielen von Backgammon, programmiert vom Entwickler Gerald Tesauro im Jahr 1992. Die einzige direkte Belohnung für das Programm ist das Ergebnis am Ende eines Spiels. Eine optimierte Version des Programms mit einer 2-Züge-Vorausschau wurde mit 1,5 Millionen Spielen gegen sich selbst trainiert. Es besiegte damit Weltklassespieler und spielt so gut wie die drei besten menschlichen Spieler. \cite[304]{Ertel}''

Aus dem oberen Zitat von Wolfgang Ertel und den Ausführungen von Russell und Norvig \cite[982]{Russell} zum Thema TD-Gammon können wir folgendes entnehmen: \\

Gerald Tesauro gelang es demnach ein verstärkendes Lernverfahren zu entwickeln, welches tatsächlich fähig war, menschliche Weltklassespieler zu besiegen. Sein Lernverfahren war dem von Samuel implementierten Lernverfahren sehr ähnlich, denn er verwendete ebenfalls das TD-Lernen, eine Funktionsannäherung und einen Selbsttrainingsmodus. In einem Selbstspielmodus spielt das Lernverfahren gegen sich selbst, um seine gelernte Strategie zu verbessern. Der besondere Unterschied der beiden Lernverfahren ist, die Darstellung der Bewertungsfunktion $\hat{U}_\theta(s)$. Gerald Tesauro stellte die Bewertungsfunktion als ein vollständig verknüpftes neuronales Netz, mit einer einzigen verborgenen Schicht mit 40 Knoten dar. \\

Die erfolgreichen Anwendungen von Arthur L. Samuel und Gerald Tesauro haben gezeigt, dass es verstärkende Lernverfahren gibt, die das Lernen einer annähernd optimalen Strategie, für Dame und Backgammon, ermöglichen. \\
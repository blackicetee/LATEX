\chapter{Auswertung}
\label{cha:auswertung}

In diesem Kapitel: //TODO Einführung in das Kapitel

\section{Konvergenz des TD-Q-Lernens}

\subsection{Generalisierung oder Funktionsannäherung}
\label{subsec:Generalisierung oder Funktionsannäherung}

\paragraph{Samuels Dame-Spiel}
Arthur L. Samuel schrieb 1955 ein Programm, dass Dame spielen konnte und mit einem einfachen Lernverfahren seine Parameter verbessern konnte. Sein Programm hatte dabei jedoch Zugriff auf eine große Zahl von archivierten Spielen, bei denen jeder einzelne Zug von Experten bewertet war (Überwachtes Lernen zur Unterstützung des verstärkenden Lernens). Damit verbesserte das Programm seine Bewertungsfunktion. Um eine noch weitere Verbesserung zu erreichen, ließ Samuel dein Programm gegen sich selbst spielen. Das Credit Assignment löste er auf einfache Weise. Für jede einzelne Stellung während eines Spiels vergleicht er die Bewertung durch die Funktion B(s) mit der durch Alpha-Beta-Pruning berechneten Bewertung und verändert B(s) entsprechend. 1961 besiegte sein Dame-Programm den viertbesten Damespieler der USA. Mit dieser bahnbrechenden Arbeit war Samuel seiner Zeit um fast dreißig Jahre voraus \cite[120\psq]{Ertel}.

\paragraph{Agent mit TD-Lernen}
Die Aufgabe des Agenten mit TD-Lernen ist, dass verbessern einer gegebenen Heuristik. Unter Verwendung dieser möglicherweise verbesserten Heuristik, soll der Agent eine möglichst optimale Aktion auswählen. Der Agent verbessert die Bewertungsfunktion durch Aktualisierung bzw. Anpassung der Parameter $\theta = \theta_1, ... \theta_n$. 

\begin{equation*}
\hat{U}_\theta(s) = \theta_1 f_1(s) + \theta_2 f_2(s) + ... + \theta_n f_n(s)
\end{equation*}

\subsection{Neuronales Lernen}
\label{subsec:Neuronales Lernen}

\paragraph{TD-Gammon}
Das TD-Lernen zusammen mit einem Backpropagation-Netz mit 40 bis 80 verdeckten Neuronen wurde sehr erfolgreich angewendet in TD-Gammon, einem Programm zum Spielen von Backgammon, programmiert vom Entwickler Gerald Tesauro im Jahr 1992. Die einzige direkte Belohnung für das Programm ist das Ergebnis am Ende eines Spiels. Eine optimierte Version des Programms mit einer 2-Züge-Vorausschau wurde mit 1,5 Millionen Spielen gegen sich selbst trainiert. Es besiegte damit Weltklassespieler und spielt so gut wie die drei besten menschlichen Spieler \cite[304]{Ertel}.  

\paragraph{Sind Lernverfahren überhaupt Sinnvoll?}

\section{Gegenüberstellung der Lernverfahren}
Belastbarkeit und Grenzen der Lernverfahren ? \\
Optimale Anwendungsspiele für die Lernverfahren? \\
Bewertung der Strategien?
\paragraph{Überwachtes Lernen}
\paragraph{Wert-Iteration und dynamische Programmierung}
\paragraph{TD-Lernen}
\paragraph{Q-Lernen}
\paragraph{Funktionsannäherung}
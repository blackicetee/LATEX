\chapter{Auswertung}
\label{cha:Auswertung}
In diesem Kapitel wollen wir die Testergebnisse des vorherigen Kapitels \ref{cha:Validierung} auswerten. Die Ergebnisse der Testphase sollen uns dabei helfen die Leistungsfähigkeit und die Grenzen des TD-Q-Lernens zu beurteilen (siehe Abschnitt \ref{sec:TD-Q-Lernen - Leistung und Grenzen}). Wir werden ein, in der Literatur bereits bekanntes, Problem dieses Lernverfahrens mit den Testergebnissen belegen und Lösungsmöglichkeiten besprechen. Diese Lösungsmöglichkeiten sind ebenfalls aus der Literatur entnommen und sie stellen herausragende Erfolge auf dem Gebiet des verstärkenden Lernens dar (siehe Abschnitt \ref{sec:Ausblick}). \\

\section{TD-Q-Lernen - Leistung und Grenzen}
\label{sec:TD-Q-Lernen - Leistung und Grenzen}
In diesem Abschnitt wollen wir Bezug auf die Testergebnisse des vorherigen Kapitels \ref{cha:Validierung} Validierung nehmen. Die Testergebnisse sollen Aussagen über die Konvergenz und über die benötigte Rechenzeit der gelernten Strategien in den verschiedenen Testphasen ermöglichen. Die Konvergenz bezieht sich auf eine Annäherung der vom TD-Q-Agenten gelernten Strategie an eine unbekannte optimale Strategie. \\

Wolfgang Ertel schreibt über die allgemeine Konvergenz des Q-Lernens (TD-Q-Lernens) (vgl. \cite[299]{Ertel}): \\

Das Q-Lernen konvergiert für ein konkretes Beispiel und allgemein, zu einer optimalen Strategie, wenn jedes Zustands-Aktions-Paar unendlich oft besucht wird. Konkret konvergiert der Wert $\hat{Q}_n(s,a)$ für alle Werte von s und a gegen $Q(s,a)$ für $n \rightarrow \infty$, mit $n$ gleich der Anzahl der Aktualisierungen des Q-Werts ($\hat{Q}_n(s,a)$).

\subsection{TD-Q-Lernen Konvergenz (Leistungsfähigkeit)}
In diesem Abschnitt analysieren wir die Testergebnisse der einzelnen Lernphasen, für 9 und 16 Spielfelder Tic Tac Toe. Wir beurteilen die Leistung der einzelnen gelernten Strategien, aufgrund der Testspiele gegen den Zufallsagenten und den Heuristik Agenten. Wir wollen versuchen die oben zitierte Aussage von Wolfgang Ertel, über die Konvergenz, mit unseren Testergebnissen zu untermauern.  \\

Die Testergebnisse des vorausschauenden Heuristik Agenten gegen den Zufallsagenten sind eindeutig. Die kleinste Gewinnquote des Heuristik Agenten erfüllt, mit 64\% Gewinnchance, das Testkriterium (siehe Abschnitt \ref{Quantifizierung der Ziele}). Die größte Gewinnquote des von uns implementierten Tic Tac Toe vorausschauenden Heuristik Agenten ist 100\%, d.h. von 100 Testspielen in einem 16 Spielfelder Tic Tac Toe, wenn der Zufallsagent nachzieht, gewinnt der Heuristik Agent alle 100 Testspiele. \\

Die Testspiele der gelernten Strategien gegen den vorausschauenden Heuristik Agent sind sehr einseitig ausgefallen. Jede gelernte Strategie, egal ob für das 9 oder 16 Spielfelder Tic Tac Toe, spielt gegen die nicht gelernte Strategie des vorausschauenden Heuristik Agenten immer das selbe Spiel. Der vorausschauende Heuristik Agent führt ebenfalls immer die selben Spielzüge aus. Die folge daraus ist, beide Agenten spielen in 100 Testspielen immer die exakt gleiche Partie mit dem selben Spielergebnis. Die gelernten Strategien haben es zudem nicht geschafft, innerhalb von maximale 10.000 Trainingsspielen, eine Strategie zu lernen, die den vorausschauenden Heuristik Agenten besiegt. \\

Die einzige Ausnahme war die in 1.000 Testspielen gelernte Strategie, diese erreichte 100 unentschieden gegen den nachziehenden Heuristik Agent (jeweils im 9 und 16 Spielfelder Tic Tac Toe). Alle 100 Testspiele wurden von beiden Agenten exakt gleich gespielt. Für die Tests wäre es vermutlich interessanter einen weiteren Trainingsmodus für den TD-Q-Agenten zu implementieren. Ein Trainingsmodus der nicht gegen sich selbst spielt, sondern gegen Andere Agenten. Die Testergebnisse gegen den Heuristik Agenten würden dann wahrscheinlich auch variieren, dies ist jedoch kein Ziel dieser Arbeit. \\

Die Testergebnisse in Spielen gegen den Zufallsagenten sind ebenfalls kritisch zu betrachten, denn sie hängen offensichtlich vom Zufall ab. Wir können die Testergebnisse demnach nur berücksichtigen, wenn diese eindeutig sind, d.h. die gelernten Strategien müssten hohe Gewinnquoten und niedrige Verlustquoten erreichen. Wir versuchen trotzdem die Leistungsfähigkeit des TD-Q-Lernens, über die Testergebnisse, gegen den Zufallsagenten, herzuleiten. \\

\myparagraph{Tic Tac Toe - 9 Spielfelder - Lernen gegen Zufall}
Testergebnisse des TD-Q Agenten gegen den nachziehenden Zufallsagenten: \\
Nach Abschluss der Lernphase 1 gewinnt er 73 und verliert 16 Testspiele. \\
Nach Abschluss der Lernphase 2 gewinnt er 79 und verliert 15 Testspiele. \\ 
Nach Abschluss der Lernphase 3 gewinnt er 92 und verliert 8 Testspiele. \\

Testergebnisse des TD-Q Agenten gegen den beginnenden Zufallsagenten: \\
Nach Abschluss der Lernphase 1 gewinnt er 37 und verliert 59 Testspiele. \\
Nach Abschluss der Lernphase 2 gewinnt er 41 und verliert 44 Testspiele. \\ 
Nach Abschluss der Lernphase 3 gewinnt er 49 und verliert 38 Testspiele. \\

Die in 10.000 Trainingsspielen gelernte Strategie des TD-Q Agenten hat deutlich bessere Gewinn- und Verlustquoten  als die in 100 Trainingsspielen gelernte Strategie. Die in 1.000 Trainingsspielen gelernte Strategie hat bessere Gewinn- und Verlustquoten als die in 100 Trainingsspielen gelernte Strategie, aber schlechtere Gewinn- und Verlustquoten als die in 10.000 Trainingsspielen gelernte Strategie.\\

Wir können aus den oberen Testergebnissen ableiten, dass die Leistungsfähigkeit (also die Spielstärke hinsichtlich einer höheren Gewinnquote und einer Niedrigen Verlustquote) ansteigt, je mehr Trainingsspiele der TD-Q-Lernende Agent durchführen konnte. Die gelernten Strategien nach 10.000 Trainingsspielen erbrachten die höchste Gewinnquote (92\%) für ein 9 Spielfelder Tic Tac Toe, wenn der TD-Q Agent gegen den nachziehenden Zufallsagenten spielt. Die Testergebnisse der Lernphasen zeigen sehr gut die Auswirkungen, die die Anzahl der Trainingsspiele auf die Konvergenz der gelernten Strategie hat. \\

Obwohl es praktisch nicht möglich ist unendlich viele Trainingsspiele durchzuführen, konnten wir zeigen, dass die gelernten Strategien eher zu einer optimalen Strategie konvertieren, je mehr Trainingsspiele durchgeführt werden. Die oben zitierte Aussage von Wolfgang Ertel, über die Konvergenz, trifft für diese Testergebnisse demnach annähern zu. \\

\myparagraph{Tic Tac Toe - 16 Spielfelder - Lernen gegen Zufall}
Testergebnisse des TD-Q Agenten gegen den nachziehenden Zufallsagenten: \\
Nach Abschluss der Lernphase 1 gewinnt er 51 und verliert 42 Testspiele. \\
Nach Abschluss der Lernphase 2 gewinnt er 48 und verliert 34 Testspiele. \\
Nach Abschluss der Lernphase 3 gewinnt er 57 und verliert 18 Testspiele. \\

Testergebnisse des TD-Q Agenten gegen den beginnenden Zufallsagenten: \\
Nach Abschluss der Lernphase 1 gewinnt er 52 und verliert 22 Testspiele. \\
Nach Abschluss der Lernphase 2 gewinnt er 60 und verliert 23 Testspiele. \\ Nach Abschluss der Lernphase 3 gewinnt er 49 und verliert 32 Testspiele. \\

Wir können aus den oberen Testergebnissen ablesen, dass die Leistungsfähigkeit der gelernten Strategie nicht unbedingt ansteigt, je mehr Trainingsspiele das TD-Q-Lernverfahren absolviert. Beginnt der TD-Q Agent, dann ergibt sich, für die in 1.000 Trainingsspielen gelernte Strategie, eine Siegesquote von 60\% und eine Verlustquote von 23\%. Die in 10.000 Trainingsspielen gelernte Strategie erreicht nur eine Gewinnquote von 49\% und eine Verlustquote von 32\%. Beginnt wiederum der Zufallsagent, dann ergibt sich für die in 10.000 Trainingsspielen gelernte Strategie eine Siegesquote von 57\% und eine Verlustquote von 18\%. Die in 1.000 Trainingsspielen gelernte Strategie erreicht nur eine Gewinnquote von 48\% und eine Verlustquote von 18\%. \\

Die Testergebnisse für das 16 Spielfelder Tic Tac Toe zeigen keine deutliche Verbesserung der Strategien abhängig von der Anzahl der durchgeführten Trainingsspiele des TD-Q Agenten. Wir wollen nachfolgend klären, warum die Testergebnisse der einzelnen Lernphasen, für das 16 Spielfelder Tic Tac Toe, keine eindeutige Leistungssteigerung aufzeigen, obwohl die Trainingsspiele in jeder Lernphase um Faktor 10 erhöht wurden. \\

\subsection{Fluch der Dimensionalität (Grenzen des TD-Q-Lernens)}
''Trotz der Erfolge in den letzten Jahren bleibt das Lernen durch Verstärkung ein sehr attraktives Forschungsgebiet der KI, nicht zuletzt deshalb, weil auch die besten heute bekannten Lernalgorithmen bei hochdimensionalen Zustands- und Aktionsräumen wegen ihrer gigantischen Rechenzeit immer noch nicht praktisch anwendbar sind. \cite[305]{Ertel}''\\

Wolfgang Ertel beschreibt in diesem Zitat den Fluch der Dimensionalität, als großes Problem der heutigen Lernalgorithmen. Auch Russell und Norvig erwähnen dieses Problem:\\

''Bisher sind wir davon ausgegangen, dass die Nutzenfunktionen und die von den Agenten gelernten Q-Funktionen in tabellarischer Form mit einem Ausgabewert für jedes Eingabetupel vorliegen. Ein solcher Ansatz funktioniert ausreichend gut für kleine Zustandsräume, aber die Zeit bis zur Konvergenz und (für ADP) die Zeit pro Iteration steigt mit wachsendem Raum rapide an. \cite[975]{Russell}''\\

Stellen wir uns einen Suchbaum vor (ähnlich der Abbildung \ref{fig:minimax_tictactoe} aus dem Abschnitt Minimax-Suche), der alle Aktionen in jedem Zustand eines 9 Spielfelder Tic Tac Toe Spiels abbildet. Das Spielbrett eines 9 Spielfelder Tic Tac Toe's ist 3 mal 3 Spielfelder groß, die Dimension des 9 Spielfelder Tic Tac Toes ist also gleich 3. Der Verzweigungsfaktor des Suchbaums ist direkt nach dem Wurzelknoten gleich 9. Erhöhen wir die Tiefe des Suchbaums um 1, dann verringert sich der Verzweigungsfaktor ebenfalls um 1 (gilt für Reversi und Tic Tac Toe). Der Verzweigungsfaktor symbolisiert den Aktionsraum der in einem bestimmten Spielzustand vorhanden ist. Jeder Knoten (auch Blattknoten und Wurzelknoten) ist ein möglicher Spielzustand. Die Gesamtheit aller Spielzustände bildet den Zustandsraum ab. \\

Wenn wir die redundanten Spielzustände nicht ausschließen und den Fakt nicht beachten das nicht jeder Baumpfad eine maximale Länge von 9 hat (9 Spielfelder Tic Tac Toe kann schon in einer Tiefe von 5 terminieren), dann hat der Spielbaum geschätzt 362880 Spielzustände ($9 * 8 * 7 * 6 * 5 * 4 * 3 * 2$ entspricht 9 Fakultät!). Erhöhen wir die Dimension des Tic Tac Toe Spiels auf 4, dann erhalten wir das 16 Spielfelder Tic Tac Toe. Berechnen wir die Spielzustände für das 16 Spielfelder Tic Tac Toe unter den gleichen Voraussetzungen wie bei dem 9 Spielfelder Tic Tac Toe, dann erhalten wir ungefähr 20922789888000 Spielzustände. Diese Anzahl der Spielzustände entsprechen nicht der tatsächlichen Anzahl, sie sollen hier nur als grobe Schätzung und zur Veranschaulichung dienen (keine Garantie der Korrektheit der Werte). \\

Wir können aus diesen beiden Spielzustandsschätzungen schlussfolgern, dass eine Erhöhung der Dimension einen enormen Effekt auf die Vergrößerung des Zustands- und Aktionsraums hat. Vermutlich meint Wolfgang Ertel genau dieses Verhältnis, wenn er von Fluch der Dimensionalität spricht. \\

\myparagraph{Rechenzeit der Lernphasen}
Wir können die in der Testphase ermittelten Rechenzeitwerte der einzelnen Lernphasen verwenden, um den Fluch der Dimensionalität aufzuzeigen. \\

Rechenzeit des TD-Q-Lernens für 9 Spielfelder Tic Tac Toe: 
\begin{itemize}
\item 100 Trainingsspiele ungefähr 5 Minuten 
\item 1.000 Trainingsspiele ungefähr 25 Minuten
\item 10.000 Trainingsspiele ungefähr 180 Minuten
\end{itemize}

Rechenzeit des TD-Q-Lernens für 16 Spielfelder Tic Tac Toe: 
\begin{itemize}
\item 100 Trainingsspiele ungefähr 25 Minuten
\item 1.000 Trainingssiele ungefähr 180 Minuten
\item 10.000 Trainingsspiele ungefähr 1440 Minuten
\end{itemize}

Die einzelnen Testergebnisse zeigen deutlich eine Rechenzeiterhöhung der Lernphasen des 9 Spielfelder Tic Tac Toe's im Gegensatz zum 16 Spielfelder Tic Tac Toe, d.h. mit steigender Dimension erhöht sich die Rechenzeit für das TD-Q-Lernen erheblich. Der TD-Q-Agent benötigt 180 Minuten um eine 9 Spielfelder Tic Tac Toe Strategie, in 10.000 Trainingsspielen, zu lernen. Dahingegen benötigt er bereits 1440 Minuten um eine 16 Spielfelder Tic Tac Toe Strategie, in ebenfalls 10.000 Trainingsspielen, zu lernen. \\

\subsection{Zusammenfassung}
Die Auswertung hat bereits ergeben, dass die gelernten Strategien, für das 16 Spielfelder Tic Tac Toe, nicht so Leistungsfähig sind, wie die gelernten Strategien für das 9 Spielfelder Tic Tac Toe. Grund dafür ist der enorm erhöhte Spielzustandsraum des 16 Spielfelder Tic Tac Toe's gegenüber dem 9 Spielfelder Tic Tac Toe. In 10.000 Trainingsspielen werden die ungefähr 362880 geschätzten Spielzustände des 9 Spielfelder Tic Tac Toe's wesentlich öfter besucht als in einem 16 Spielfelder Tic Tac Toe mit ungefähr 20922789888000 geschätzten Spielzuständen. \\

Wir haben bereits die allgemeine Konvergenzbedingung des Q-Lernens (TD-Q-Lernen) von Wolfgang Ertel zitiert. Demnach konvergiert die gelernte Strategie nur zu einer optimalen Strategie, wenn jedes Zustands-Aktions-Paar unendlich oft besucht wird. Da der geschätzte Zustandsraum des 9 Spielfelder Tic Tac Toe's viel kleiner ist, als der geschätzte 16 Spielfelder Tic Tac Toe Zustandsraum, konvergiert die gelernte Strategie annähernd für das 9 Spielfelder Tic Tac Toe in 10.000 Trainingsspielen, aber nicht für das 16 Spielfelder Tic Tac Toe. Daraus folgt: 10.000 Trainingsspiele reichen nicht aus, um die einzelnen Spielzustände des 16 Spielfelder Tic Tac Toe's für eine Konvergenz ausreichend oft zu besuchen. \\  

Aus diesen Gründen untersuchen wir die Lernfähigkeit und Grenzen des TD-Q-Lernens nur für 9 und 16 Spielfelder Tic Tac Toe Spiele und nicht für das 64 Spielfelder Reversi. 10.000 Trainingsspiele für das Reversi Strategiespiel würden Tage oder Wochen an Rechenzeit benötigen und letztlich würden 10.000 Trainingsspiele für den Reversi Zustandsraum nicht ausreichen, um eine annähernd optimale Reversi Strategie zu lernen. \\

Die bisherigen Ergebnisse dieser Arbeit bestätigen somit die nachfolgende Aussagen von Wolfgang Ertel: \\

''Die weltbesten Schachcomputer arbeiten bis heute immer noch ohne Lernverfahren. Dafür gibt es zwei Gründe. Einerseits benötigen die bis heute entwickelten Verfahren zum Lernen durch Verstärkung bei großen Zustandsräumen noch sehr viel Rechenzeit. Andererseits sind aber die manuell erstellten Heuristiken der Hochleistungsschachcomputer schon sehr stark optimiert. Das heißt, dass nur ein sehr gutes Lernverfahren noch zu Verbesserungen führen kann. \cite[120]{Ertel}''\\

\section{Lösungen für das Dimensionalitätsproblem}
\label{sec:Ausblick}
Im letzten Abschnitt dieser Arbeit wollen wir auf die sehr guten Lernverfahren eingehen, die Wolfgang Ertel in seiner Aussage gerade eben erwähnt hat. Die beiden vielleicht bisher erfolgreichsten lernenden Programme. Das Dame-Spiel von Arthur L. Samuel aus dem Jahre 1955 und TD-Gammon von Gerald Tesauro aus dem Jahre 1992. 

\subsection{Samuels-Dame-Spiel}
''Arthur L. Samuel schrieb 1955 ein Programm, dass Dame spielen konnte und mit einem einfachen Lernverfahren seine Parameter verbessern konnte. Sein Programm hatte dabei jedoch Zugriff auf eine große Zahl von archivierten Spielen, bei denen jeder einzelne Zug von Experten bewertet war (Überwachtes Lernen zur Unterstützung des verstärkenden Lernens). Damit verbesserte das Programm seine Bewertungsfunktion. Um eine noch weitere Verbesserung zu erreichen, ließ Samuel dein Programm gegen sich selbst spielen. Das Credit Assignment löste er auf einfache Weise. Für jede einzelne Stellung während eines Spiels vergleicht er die Bewertung durch die Funktion B(s) mit der durch Alpha-Beta-Pruning berechneten Bewertung und verändert B(s) entsprechend. 1961 besiegte sein Dame-Programm den viertbesten Damespieler der USA. Mit dieser bahnbrechenden Arbeit war Samuel seiner Zeit um fast dreißig Jahre voraus. \cite[120\psq]{Ertel}''\\

Wir wollen nachfolgend den unterschied zwischen dem in dieser Arbeit implementierten Lernverfahren und dem von Samuel implementierten Lernverfahren erläutern (vgl. \cite[976]{Russell}): \\

Russell und Norvig beschreiben das Lernverfahren von Samuel als \textbf{Funktionsannäherung}. Der unterschied zwischen Samuels Lernalgorithmus und dem von uns implementierten TD-Q-Algorithmus ist, dass Samuel eine andere Darstellung als eine Suchtabelle (die Tabelle die wir mit Q(s,a) gekennzeichnet hatten) für die Q-Funktion verwendet. Arthur Samuels Dame Spiel verwendete also eine parametrisierte Bewertungsfunktion in der Form: \\
\begin{equation*}
\hat{U}_\theta(s) = \theta_1 f_1(s) + \theta_2 f_2(s) + ... + \theta_n f_n(s),
\end{equation*}

um die Q-Funktion darzustellen. Samuel verwendet für das Lernen der einzelnen Parameter ($\theta = \theta_1, ... \theta_n$) der Bewertungsfunktion $\hat{U}_\theta(s)$ eine abgewandelte Form, des in dieser Arbeit implementierten TD-Q-Lernens (vgl. \cite[981]{Russell}). Dieses Vorgehen ermöglichte es Samuels Lernalgorithmus die Q-Funktion, statt in einer $10^{40}$ Tabelle, durch etwa $n = 20$ Parameter zu charakterisieren. Dies ist gegenüber der Tabelle (unsere Darstellung der Q-Funktion) die unser TD-Q Agent in dieser Arbeit lernt, eine riesige Komprimierung. \\

\subsection{TD-Gammon}
\label{subsec:TD-Gammon}
''Das TD-Lernen zusammen mit einem Backpropagation-Netz mit 40 bis 80 verdeckten Neuronen wurde sehr erfolgreich angewendet in TD-Gammon, einem Programm zum Spielen von Backgammon, programmiert vom Entwickler Gerald Tesauro im Jahr 1992. Die einzige direkte Belohnung für das Programm ist das Ergebnis am Ende eines Spiels. Eine optimierte Version des Programms mit einer 2-Züge-Vorausschau wurde mit 1,5 Millionen Spielen gegen sich selbst trainiert. Es besiegte damit Weltklassespieler und spielt so gut wie die drei besten menschlichen Spieler. \cite[304]{Ertel}''

Aus dem oberen Zitat von Wolfgang Ertel und den Ausführungen von Russell und Norvig \cite[982]{Russell} zum Thema TD-Gammon können wir folgendes feststellen: \\

Gerald Tesauro gelang es demnach ein verstärkendes Lernverfahren zu entwickeln, welches tatsächlich fähig war, menschliche Weltklassespieler zu besiegen. Sein Lernverfahren war dem von Samuel implementierten Lernverfahren sehr ähnlich, denn er verwendete ebenfalls das TD-Lernen, eine Funktionsannäherung und einen Selbsttrainingsmodus. In einem Selbstspielmodus spielt das Lernverfahren gegen sich selbst, um seine gelernte Strategie zu verbessern. Der besondere Unterschied der beiden Lernverfahren ist, die Darstellung der Bewertungsfunktion $\hat{U}_\theta(s)$. Gerald Tesauro stellte die Bewertungsfunktion als ein vollständig verknüpftes neuronales Netz mit einer einzigen verborgenen Schicht mit 40 Knoten dar. \\

Die erfolgreichen Anwendungen von Arthur L. Samuel und Gerald Tesauro haben gezeigt, dass es verstärkende Lernverfahren gibt, die das Lernen einer annähernd optimalen Strategie, für Dame und Backgammon, ermöglichen. \\
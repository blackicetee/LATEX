\chapter{Projektvision}
\label{cha:projektvision}

Viele Menschen spielen gerne Strategiespiele gegen andere Menschen oder gegen einen Computer. \\
Sie veranstalten große Meisterschaften in Schach und Poker. ''... Schach - zumindest in der Form des Turnierschachs - ist heute unbestreitbar als Sport anzusehen ... \cite{Weyer}'' Schach ist demnach nicht nur ein Spiel, sondern auch ein anerkannter Turniersport. \\

Der Reiz eines Strategiespiels ist vermutlich die Entwicklung und Verbesserung der Strategie. Der Mensch lernt seine Strategien durch ständiges trainieren, verlieren, siegen, analysieren und anpassen. Er kann seine Strategie auch aus Büchern oder von einem Lehrer lernen.
Eine Strategie, die sich sehr oft in der Praxis bewährt hat und viele wichtige Aspekte und Spielregeln beachtet, wird die Gewinnchancen eines Spielers verbessern. \\

''Zum ersten mal hat der seit zehn Jahren amtierende Schachweltmeister Garri Kasparow, den viele für den stärksten Spieler aller Zeiten halten, eine normale Turnierpartie gegen einen Schachcomputer verloren. 
In Philadelphia musste der Champion in der ersten von sechs Partien eines Zweikampfs gegen das auf einem IBM-Großrechner laufende Schachprogramm Deep Blue nach 37 Zügen die Waffen strecken. \cite{Neander}''\\

Dementsprechend kündigt sich eine Veränderung in den Turnieren und Meisterschaften der Strategiespiele an. Immer mehr menschliche Meister der Strategien werden von Computern besiegt. \\

Wir wollen daher folgende Fragen in dieser Arbeit behandeln:\\
Wie spielt ein Computer Strategiespiele oder wie entwickelt er Strategien? Lernen die Computer ihre Strategien oder werden ihnen explizit Strategien vorgegeben? Werden lernende Computerprogramme, in nächster Zeit, Turniere und Meisterschaften gewinnen? 
\newpage

\section{Zielsetzung}
Das Ziel der Arbeit ist es, den bereits existierenden Lernalgorithmus des TD-Q-Lernens zu implementieren und dessen Leistungsfähigkeit und Grenzen zu untersuchen. Das daraus resultierende Lernverfahren, soll eigenständig und automatisch Strategien für zwei Computerspiele erlernen. Wir bezeichnen die Implementierung des TD-Q-Lernens fortan als TD-Q-Agenten. \\

Der TD-Q-Agent soll Strategien für die selbst programmierten Computerspiele Tic Tac Toe und Reversi erlernen. Wir werden die Strategiespiele Tic Tac Toe mit 9 Spielfeldern, Tic Tac Toe mit 16 Spielfeldern und Reversi mit 64 Spielfeldern eigenständig implementieren. \\

Wir beurteilen die Leistungsfähigkeit des implementierten TD-Q-Agenten, anhand von Testspielen gegen andere Agenten mit unterschiedlichen Strategien. Wir werden dafür innerhalb dieser Arbeit einen Zufallsagenten und einen vorausschauenden Heuristik Agent entwickeln und implementieren. \\

\section{Quantifizierung der Ziele}
\label{sec:Quantifizierung der Ziele}

Wir unterteilen das Lernen des TD-Q-Agenten in drei Phasen. In der ersten Phase lernt der TD-Q-Agent in 100 Trainingsspielen eine Strategie. Trainingsspiel bedeutet: Der TD-Q-Agent spielt und lernt gegen sich selbst. Wir erhören die Anzahl der Trainingsspiele in der zweiten Phase auf 1.000 und in der dritten Phase auf 10.000 Trainingsspiele. \\

Für jede Lernphase entsteht eine eigene Strategie. Insgesamt lernt der TD-Q-Agent demnach 9 Strategien. 3 Strategien für das 9 Spielfelder Tic Tac Toe, 3 Strategien für das 16 Spielfelder Tic Tac Toe und 3 Strategien für das 64 Spielfelder Reversi. \\

Nach Abschluss jeder Lernphase wird der TD-Q-Agent mit der gelernten Strategie, in genau 100 Testspielen, gegen den vorausschauenden Heuristik Agenten und den Zufallsagenten spielen. Der Heuristik Agent wird ebenfalls in 100 Testspielen gegen den Zufallsagenten spielen. \\

\section{Aufbau des Projekts}
Zuerst werden die Spielregeln der beiden ausgesuchten Computerspiele definiert. Die Computerspiele und entsprechende Tests werden programmiert. Die Programmierung der Computerspiele und der Tests wird nicht explizit aufgeführt, kann jedoch der beiliegenden Software-CD entnommen werden. \\

In den nächsten beiden Schritten werden die Grundlagen für die Implementierung des Heuristik Agenten und des TD-Q-Agenten ermittelt und erstellt. \\

Für die Erstellung der Agenten werden danach die Anforderungen definiert und anschließend modelliert. \\

Die definierten Anforderungen werden Implementiert. Der Algorithmus der das vorausschauen des Heuristik Agenten realisiert und der Algorithmus der das eigenständige Lernen des TD-Q-Agenten realisiert werden ausführlich in dieser Arbeit beschrieben. Die kompletten implementierten Anforderungen und die benötigten Algorithmen für die Agenten sind auf der Software-CD vorhanden. \\

Anschließend beginnen die geplanten Trainingsphasen und Testspiele. \\

Der letzte Schritt ist die Auswertung der Testergebnisse und die Beurteilung der Leistungsfähigkeit der Agenten. \\

\section{Ergebnisse}
\label{sec:Ergebnisse}

Innerhalb dieser Arbeit wollen wir verschiedene Aussagen der Literatur bestätigen: Die Dimensionalität ist ein großes Problem der Lernalgorithmen. Lernverfahren sind momentan noch ungeeignet für das Lernen von komplexen Strategiespielen und sie werden von manuell optimierten Heuristiken dominiert.

Dahingehend stellen wir folgende Hypothesen auf: \\

\begin{enumerate}
\item Der Heuristik Agent wird in beiden Strategiespielen gegen den lernenden Agenten mindestens 60\% aller Testspiele gewinnen.
\item Das TD-Q-Lernen kann, innerhalb von maximal 10.000 Trainingsspielen gegen sich selbst, keine Strategie entwickeln, die in 100 Testspielen häufiger Gewinnt, als der vorausschauende Heuristik Agent.
\item Die Konvergenzgeschwindigkeit, ist die Geschwindigkeit, die das TD-Q-Lernen für das Lernen einer optimalen Strategie benötigt. Sie ist voraussichtlich stark von der Dimensionalität des Ausgangsproblems abhängig, d.h. das TD-Q-Lernen ist nur auf sehr einfache bzw. niedrig dimensionierte Probleme anwendbar.
\end{enumerate}



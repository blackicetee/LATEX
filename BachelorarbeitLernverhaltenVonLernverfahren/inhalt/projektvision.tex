\chapter{Projektvision}
\label{cha:projektvision}

Viele Menschen spielen gerne Strategiespiele gegen andere Menschen oder gegen einen Computer. \\
Sie veranstalten große Meisterschaften in Schach und Poker. ''... Schach - zumindest in der Form des Turnierschachs - ist heute unbestreitbar als Sport anzusehen ... \cite{Weyer}'' Schach ist demnach nicht nur ein Spiel, sondern auch ein anerkannter Turniersport. \\

Der Reiz eines Strategiespiels ist vermutlich die Entwicklung und Verbesserung der Strategie. Der Mensch lernt seine Strategien durch ständiges trainieren, verlieren, siegen, analysieren und anpassen. Er kann seine Strategie auch aus Büchern oder von einem Lehrer lernen.
Eine Strategie, die sich sehr oft in der Praxis bewährt hat und viele wichtige Aspekte und Spielregeln beachtet, wird die Gewinnchancen eines Spielers verbessern. \\

''Zum ersten mal hat der seit zehn Jahren amtierende Schachweltmeister Garri Kasparow, den viele für den stärksten Spieler aller Zeiten halten, eine normale Turnierpartie gegen einen Schachcomputer verloren. 
In Philadelphia musste der Champion in der ersten von sechs Partien eines Zweikampfs gegen das auf einem IBM-Großrechner laufende Schachprogramm Deep Blue nach 37 Zügen die Waffen strecken. \cite{Neander}''\\

Dementsprechend kündigt sich eine Veränderung in den Turnieren und Meisterschaften der Strategiespiele an. Immer mehr menschliche Meister der Strategien werden von Computern besiegt. \\

Wir wollen daher folgende Fragen in dieser Arbeit behandeln:\\
Wie spielt ein Computer Strategiespiele oder wie entwickelt er Strategien? Lernen die Computer ihre Strategien oder werden ihnen explizit Strategien vorgegeben? Werden lernende Computerprogramme, in nächster Zeit, Turniere und Meisterschaften gewinnen? 
\newpage

\section{Zielsetzung}
Das Ziel der Arbeit ist es, den bereits existierenden Lernalgorithmus des TD-Q-Lernens zu implementieren und dessen Leistungsfähigkeit und Grenzen zu untersuchen. Das daraus resultierende Lernverfahren, soll eigenständig und automatisch Strategien für zwei Computerspiele erlernen. Wir bezeichnen die Implementierung des TD-Q-Lernens fortan als TD-Q-Agenten. \\

Der TD-Q-Agent soll Strategien für die selbst programmierten Computerspiele Tic Tac Toe und Reversi erlernen. Wir werden die Strategiespiele Tic Tac Toe mit 9 Spielfeldern, Tic Tac Toe mit 16 Spielfeldern und Reversi mit 64 Spielfeldern eigenständig implementieren. \\

Wir beurteilen die Leistungsfähigkeit des implementierten TD-Q-Agenten, anhand von Testspielen gegen andere Agenten mit unterschiedlichen Strategien. Wir werden dafür innerhalb dieser Arbeit einen Zufallsagenten und einen vorausschauenden Heuristik Agent entwickeln und implementieren. \\

\section{Quantifizierung der Ziele}
\label{sec:Quantifizierung der Ziele}

Wir unterteilen das Lernen des TD-Q-Agenten in drei Phasen. In der ersten Phase lernt der TD-Q-Agent in 100 Trainingsspielen eine Strategie. Trainingsspiel bedeutet: Der TD-Q-Agent spielt und lernt gegen sich selbst. Wir erhören die Anzahl der Trainingsspiele in der zweiten Phase auf 1.000 und in der dritten Phase auf 10.000 Trainingsspiele. \\

Für jede Lernphase entsteht eine eigene Strategie. Insgesamt lernt der TD-Q-Agent demnach 9 Strategien. 3 Strategien für das 9 Spielfelder Tic Tac Toe, 3 Strategien für das 16 Spielfelder Tic Tac Toe und 3 Strategien für das 64 Spielfelder Reversi. \\

Nach Abschluss jeder Lernphase wird der TD-Q-Agent mit der gelernten Strategie, in genau 100 Testspielen, gegen den vorausschauenden Heuristik Agenten und den Zufallsagenten spielen. Der Heuristik Agent wird ebenfalls in 100 Testspielen gegen den Zufallsagenten spielen. \\

\section{Aufbau des Projekts}
Zuerst werden die Spielregeln der beiden ausgesuchten Computerspiele definiert. Die Computerspiele und entsprechende Tests werden programmiert. Die Programmierung der Computerspiele und der Tests wird nicht explizit aufgeführt, kann jedoch der beiliegenden Software-CD entnommen werden. \\

In den nächsten beiden Schritten werden die Grundlagen für die Implementierung des Heuristik Agenten und des TD-Q-Agenten ermittelt und erstellt. \\

Für die Erstellung der Agenten werden danach die Anforderungen definiert und anschließend modelliert. \\

Die definierten Anforderungen werden Implementiert. Der Algorithmus der das vorausschauen des Heuristik Agenten realisiert und der Algorithmus der das eigenständige Lernen des TD-Q-Agenten realisiert werden ausführlich in dieser Arbeit beschrieben. Die kompletten implementierten Anforderungen und die benötigten Algorithmen für die Agenten sind auf der Software-CD vorhanden. \\

Anschließend beginnen die geplanten Trainingsphasen und Testspiele. \\

Der letzte Schritt ist die Auswertung der Testergebnisse und die Beurteilung der Leistungsfähigkeit der Agenten. \\

\section{Ergebnisse}

Innerhalb dieser Arbeit wollen wir verschiedene Aussagen der Literatur bestätigen: Die Dimensionalität ist ein großes Problem der Lernalgorithmen. Lernverfahren sind momentan noch ungeeignet für das Lernen von komplexen Strategiespielen und sie werden von manuell optimierten Heuristiken dominiert.





\section{Realisierung Heuristik Agent}
%Realisierung nicht lernender Agent / feste Strategie
Der vorausschauende Heuristik Agent ist ein nicht lernender Agent der eine von uns festgelegte Strategie erhält. Die Strategie besteht aus einer Stellungsbewertung (Heuristik) und einer 2 Spielzüge vorausschauenden Spielbaumsuche. Wir werden in Abschnitt \ref{Spieltheorie} Spieltheorie die Theoretischen Grundlagen erklären, die wir für die Implementierung des vorausschauenden Heuristik Agenten benötigen.  


In der Implementierung des vorausschauenden Heuristik Agenten ist, neben der Bewertungsfunktion, noch ein anderes Verfahren enthalten. Das Suchbaumverfahren für 2-Personenspiele. Dieses Verfahren durchsucht einen Spielbaum nach der bestmöglichen Aktion (einem Spielzug) in einem gegebenen Zustand. Ein Zustand oder Spielzustand ist eine Spielsituation bzw. eine Stellung der Spielfiguren auf dem Spielfeld. \\

Das Problem der Suchverfahren ist die Dimensionalität bzw. Komplexität des Ausgangsproblems. Suchbaumverfahren können für sehr einfache Probleme relativ schnell eine optimale Aktion finden. Die Größe des Suchbaums wächst exponentiell mit der Komplexität des Problems, d.h. die Laufzeit des Suchbaumverfahrens ohne Erweiterungen könnte für das Strategiespiele Tic Tac Toe nicht handhabbar sein und ist für das Strategiespiel Reversi nicht handhabbar. Wir schreiben ''könnte'' bei Tic Tac Toe, weil dieses noch ein recht einfacher Vertreter der Strategiespiele ist, dahingegen ist Reversi ein komplexeres Strategiespiel. \\

Um die Diemensionalitätsproblematik zu lösen, kombinieren wir Suchbaumverfahren mit Heuristiken, wir bezeichnen diese Kombination als heuristische Suche. Eine Heuristik berechnet eine Gewinnwahrscheinlichkeit, ausgehend von einem Spielzustand. Ein Spielzustand mit einer hohen heuristischen Bewertung ist, gegenüber einem Spielzustand mit niedriger heuristischer Bewertung, zu bevorzugen. \\

Das Suchbaumverfahren muss den Suchbaum, unter Verwendung einer Heuristik, nicht mehr komplett durchsuchen. Die Suche kann in einer bestimmten Suchbaumtiefe abgebrochen werden. Das Suchbaumverfahren liefert die erste Aktion einer Aktionssequenz. Eine Aktionssequenz ist eine Folge von Aktionen und beschreibt einen Pfad im Suchbaum. Die Aktionssequenz, welche von der heuristischen Suche ausgewählt wurde, repräsentiert den Spielzustand mit der maximalen Gewinnwahrscheinlichkeit. \\

Die Qualität dieser Gewinnschätzung ist wiederum von der maximalen Suchtiefe und der Bewertungsfunktion abhängig. Eine größere Suchtiefe resultiert in einer besseren Schätzung, weil unter Umständen mehr Spielzustände berücksichtigt werden können. Die Verwendung einer Bewertungsfunktion ist keine Garantie für eine optimale Strategie. Verschiedene Bewertungsfunktionen können stark voneinander abweichende Gewinnschätzungen für Spielzustände berechnen. \\

\section{Realisierung des TD-Q lernenden Agent}
Wir stellen mehrere Lernverfahren innerhalb dieser Arbeit vor, aber wir werden nur das Q-Lernen (auch TD-Q-Lernen) implementieren und untersuchen. Das TD-Q-Lernen ist ein Lernverfahren aus dem Bereich des verstärkenden Lernens. Das TD-Q-Lernen soll es uns ermöglichen einen selbst lernenden Agenten zu programmieren. Verstärkendes Lernen (eng. reinforcement Learning) ist eine Lernkategorie des maschinellen Lernens. Problemstellungen des verstärkenden Lernens sind, u.a. das lernen von Strategiespielen, wie Schach, Reversi, Dame oder Backgammon. Der theoretische verstärkend lernende Lösungsansatz dieser Probleme ist wie folgt: ein Agent soll ein ihm unbekanntes Strategiespiel lernen (das Strategiespiel ist die unbekannte Umgebung), für einen Spielzug  (Aktion) in einer Spielsituation (Zustand) erhält der Agent eine numerische Belohnung oder Bestrafung (Verstärkung), mittels dieser Verstärkung soll der Agent ein optimales Verhalten in der ihm unbekannte Umgebung erlernen.\\

Wie realisiert das TD-Q-Lernen diesen verstärkenden Lernansatz? Das TD-Q-Lernen lernt Q-Werte für Zustand/Aktionspaare, diese Q-Werte werden bei jedem erneuten Auftreten des Zustand/Aktionspaares aktualisiert. Eine Q-Funktion ist eine Abbildung von allen möglichen Zustand/Aktionspaaren auf Q-Werte und eine Q-Funktion ist eine Möglichkeit Nutzeninformationen zu speichern \cite[974]{Russell}. Nachdem der Agent eine Q-Funktion gelernt hat, kann er mittels dieser, vermeintlich optimale Aktionen auswählen. Wir schreiben ''Vermeintlich'', weil eine gelernte Q-Funktion nicht immer zu einer optimalen Strategie konvergiert.

Wir zeige in dieser Arbeit praktisch, dass das TD-Q-Lernen ohne Erweiterungen, nur auf Probleme mit geringer Komplexität angewendet werden kann. Die Komplexität bzw. Dimensionalität des Ausgangsproblems ist ein Grund dafür, dass die gelernte Q-Funktion nicht immer zu einer optimalen Strategie konvergiert, ein anderer Grund ist die zeitliche Beschränkung durch die Realität, d.h. in der Realität können nicht unendlich viele Testspiele durchgeführt werden. Es wurde bereits empirisch belegt, dass das Q-Lernen, sollte jedes Zustand/Aktionspaar nahezu unendlich oft besucht und aktualisiert werden, immer zu einer optimalen Strategie konvergiert. Das Problem dabei ist, dass die Komplexität bzw. die Dimensionalität des Ausgangsproblems, ein exponentiellen Verhältnis zur Zustands- und Aktionsmenge hat. \\

Lernt der TD-Q Agent, z.B. innerhalb von 10.000 Testspielen eine nahezu optimale Strategie für ein Tic Tac Toe Spiel mit 3 mal 3 Dimensionen (9 Spielfelder), dann ist das TD-Q-Lernen praktisch für ein Strategiespiel bis zu dieser Dimensionalität anwendbar. Erhöhen wir die Zustands- und Aktionsdimension, z.B. bei einem 16 Spielfelder Tic Tac Toe Spiel, dann reichen selbst 1.000.000 Testspiele unter Umständen nicht mehr aus, um eine annähernd optimale Strategie zu lernen. Jede weitere Dimension erhöht außerdem die Dauer eines Trainingsspiels, d.h. für jede weitere Dimension benötigt das TD-Q-Lernverfahren erheblich mehr Testspiele, um zu einer annähernd optimalen Strategie zu konvergieren und gleichzeitig erhöht sich die Dauert jedes Testspiels für jede zusätzliche Dimension des Ausgangsproblems.

\section{Hypothesen}
\label{sec:Hypothese}
\begin{enumerate}
\item Der Heuristik Agent wird in beiden Strategiespielen gegen den lernenden Agenten mindestens 50\% aller Testspiele gewinnen.
\item Das TD-Q-Lernen kann, innerhalb von maximal 10.000 Trainingsspielen gegen sich selbst, keine Strategie entwickeln, die in 100 Testspielen häufiger Gewinnt, als die in dieser Arbeit implementierte 2-Züge vorausschauende Heuristik-Suche.
\item Das TD-Q-Lernen muss möglicherweise mehr als 10.000 Trainingsspiele gegen sich selbst spielen, um eine bessere Strategie, als die nicht lernende Strategie, zu lernen.
\item Die Konvergenzgeschwindigkeit das TD-Q-Lernen zu einer optimalen Strategie, ist möglicherweise stark von der Dimensionalität des Ausgangsproblems abhängig, d.h. genau wie die uninformierten Suchbaumverfahren, ist das TD-Q-Lernen nur auf sehr einfache bzw. niedrig dimensionale Probleme anwendbar. Konvergenzgeschwindigkeit ist die Zeit, die ein Lernverfahren benötigt, bis es eine annähernd optimale Strategie entwickelt hat.
\item Das reine TD-Q-Lernen, ohne Erweiterungen, ist möglicherweise keine geeignetes Lernverfahren für das lernen eines Strategiespiels.  
\end{enumerate}




\chapter{Implementierung}
\label{cha:Implementierung}

In diesem Kapitel: //TODO Einführung in das Kapitel

\section{Tic Tac Toe}

\section{Reversi}

\section{Suchbaumverfahren}

\section{Heuristiken}

\section{TD-Q-Lernen}

Wir haben bisher erfahren, das ein Agent durch Verstärkung in einer ihm unbekannten Umgebung eine Verhaltensstrategie $\pi(s)$ lernen kann, die für jeden Zustand s eine Aktion a vorschlägt. Wir erinnern uns, dass der Nutzen U, in einem Zustand s, unter Beachtung einer Strategie $\pi$, berechnet wurde aus der Summe aller abgeschwächten Belohnungen, für jeden besuchten Zustand, in einem Zeitintervall von $t = 0$ bis $\infty$ (siehe \ref{subsec:Optimale Taktiken} Optimale Taktiken Gleichung für den erwarteten Nutzen \ref{eq:Der erwartete Nutzen}). Dementsprechend gibt eine optimale Taktik $\pi^*(s)$ für jeden Zustand s den Nachfolgezustand mit dem größtmöglichen erwarteten Nutzen an:

\begin{equation}
\pi^*(s) = \argmax_{a \in A(s)} \sum_{s'} P(s'|s, a) U(s').
\end{equation}

Daraus folgt, dass es eine direkte Beziehung zwischen dem Nutzen eines Zustandes und dem Nutzen seiner Nachbarn gibt: Der Nutzen eines Zustandes ist der unmittelbare Gewinn für diesen Zustand plus dem erwarteten verminderten Gewinn des nächsten Zustandes , vorausgesetzt, der Agent wählt die optimale Aktion. Das bedeutet, der Nutzen eines Zustandes ist gegeben durch (siehe Bellman-Gleichung in Abschnitt \ref{subsec:Temporale Differenz Lernen}):

\begin{equation}
U(s) = R(s) + \gamma \max_{a \in A(s)} \sum_{s'} P(s'|s, a) U(s').
\end{equation}

Die aus der Bellman-Gleichung formulierbare Aktualisierungsregel, die Bellman-Aktualisierung ist Hauptbestandteil des Wert-Iteration Algorithmus. Dieses Lernverfahren wird im Rahmen dieser Arbeit jedoch nicht implementiert. Es dient hier lediglich dazu die Unterschiede zum TD-Q-Lernen Algorithmus zu erläutern. Die Wert-Iteration kommt aus der dynamischen Programmierung und ist ein sehr bedeutsames Lernverfahren des verstärkenden Lernens.

Die Aufgabe des TD-Q-Lernenden Agenten ist ebenfalls eine optimale Strategie zu entwickeln, jedoch mit dem Unterschied, dass dieser keine Nutzenfunktion U(s) lernt, sondern eine Q-Funktion. Eine Q-Funktion ist eine Abbildung von Zustands/Aktions-Paaren auf Nutzwerte. Q-Werte sind wie folgt mit Nutzwerten verknüpf \cite[973]{Russell}:

\begin{equation}
U(s) = \max_a Q(s,a).
\end{equation}

Eine Nutzenfunktion U(s) ist abhängig von den abgeschwächten Nutzwerten aller nachfolgenden Zustände. Ein TD-Agent der eine Q-Funktion lernt, braucht weder für das Lernen noch die Aktionsauswahl ein Modell der Form P(s'|s,a). Aus diesem Grund sagt man auch, das Q-Lernen ist eine modellfreie Methode \cite[974]{Russell}. \\

Was ist jedoch der Unterschied zwischen einer Belohnungsfunktion r(s, a) und einer Q-Funktion Q(s, a)? Die Funktion r(s, a) ist von der Umgebung definiert und kann vom Agenten nicht beeinflusst werden. Sollte diese Funktion dem Agenten eine numerische Verstärkung von -0,5 zuweisen, dann kann der Agent dies nicht ändern. Der Agent soll versuchen die Zusammenhänge der Zustands/Aktions-Paare zu lernen und Entscheidungen basierend auf seinen Lernerfahrungen zu treffen. Dies bezeichnen wir dann als Q-Lernen. Die vom Agenten gelernten zusammenhänge werden in Q-Werten gespeichert. Folglich wird in Q(s, a) oder Q[s, a] die gelernte Erfahrung des Agenten, für ein Zustand/Aktions-Paar, gespeichert. \\

\begin{lstlisting}[caption=TD-Q-Lernen Algorithmus, captionpos=b, label=lst:TD-Q-Lernen Algorithmus, frame=single, mathescape=true]
1  def Q-Lernen(s', r', $\alpha$, $\gamma$):
2    if istTerminalzustand(s):
3      Q[s, None] $\leftarrow$ r'
4    if s ist nicht None:
5      inkrementiere $N_{sa}$[s, a]
6      Q[s, a] $\leftarrow$ Q[s, a] + $\alpha$(N$_{sa}${s, a])
  		* (r + $\gamma max_{a'}$ Q[s', a'] - Q[s, a])
7    s, a, r $\leftarrow$ s', argmax$_{a'}$, f(Q[s', a'], N$_{sa}$), r'
8    return a
\end{lstlisting}

Der in Listing \ref{lst:TD-Q-Lernen Algorithmus} skizzierte Algorithmus ist, leicht abgewandelt, im Lehrbuch für künstliche Intelligenz von Russell und Norvig \cite[974]{Russell} zu finden. Dieser ist bereits leicht modifiziert in seiner Notation und nachdem wir die Einzelheiten des Algorithmus geklärt haben, wird die Notation weiter angepasst, bis sie dem Quellcode des Prototypen entspricht. \\

Der Q-Lernen Algorithmus verwendet einige persistente (d.h. beständige oder dauerhafte) Variablen und Entitäten. Persistent deshalb, weil sie die einzelnen Funktionsaufrufe überdauern: 
\begin{itemize}

\item \textbf{Q} ist eine Tabelle mit Aktionswerden, indiziert nach Zustand und Aktion. Der Aufruf Q[s, a] liefert z.B. einen Aktionswert (Q-Wert) für eine Aktion a in einem Zustand s. Zu beginn des Lernprozesses sind alle Werte dieser Tabelle leer.

\item \textbf{N$_{sa}$} ist eine Tabelle mit Häufigkeiten für Zustand/Aktions-Paare.  Diese ist wie Q anfangs leer. Jedes mal wenn ein Zustand/Aktions-Paar durchlaufen wird, welches bereits durchlaufen wurde, dann wird der Tabelleneintrag N$_{sa}$[s, a] inkrementiert d.h. um den Wert 1 erhöht.

\item \textbf{s} ist der vorhergehende Spielzustand, anfangs leer. Berücksichtigen wir den Zeitlichen Aspekt, dann wäre s zu einem Zeitpunkt t geschrieben s${_t}$ und ein darauffolgender Spielzustand wäre s${_t+1}$. Der direkt auf s folgende Spielzustand wird auch als s' (s Prime) bezeichnet.

\item \textbf{a} ist die vorhergehende Aktion, anfangs leer. Wird die Aktion a im Zustand s ausgeführt, dann wird der Zustand s' bzw. s$_{t+1}$ erreicht. Eine Aktion die in s' ausgeführt werden kann bezeichnen wir als a' oder a$_{t+1}$.

\item \textbf{r} ist die Belohnung die dem Agenten von der Umgebung zugeteilt wird, anfangs leer, wenn der Agent eine Aktion a in einem Zustand s ausführt. Wir können eine Funktion r(s, a) definieren. Die Funktion r(s, a) wird für die meisten Spielzustände s $\in$ S den Wert 0 liefern. Für Endzustände der jeweiligen Strategiespiele wird die Funktion r(s, a) andere Werte liefern. Ist r die Belohnung dafür Aktion a in Zustand s auszuführen, dann ist r' (r Prime) die Belohnung dafür Nachfolgeaktion a' in Nachfolgezustand s' auszuführen.
\end{itemize}

Der Q-Lernen Algorithmus bekommt folgende Eingabeparameter übergeben:

\begin{itemize}
\item \textbf{s'} ist der aktuelle Spielzustand und gleichzusetzen mit der aktuellen Wahrnehmung des Agenten. Wie bereits erklärt ist s' der Nachfolgezustand von s.

\item \textbf{r'} ist das Belohnungssignal, welches der Agent erhält, wenn er eine Aktion a' im Zustand s' ausführt. 

\item \textbf{$\alpha$} ist bestimmt über die Lernrate des Algorithmus. Der Wert von $\alpha$ ist in der Regel zwischen 0 und 1. Eine hohe Lernrate ($\alpha$ nahe 1) bedeutet, dass die Aktualisierung des Q-Werts stärker ist. Bei einer niedrigen Lernrate ist die Aktualisierung schwächer. Der Ausdruck $\alpha$(N$_{sa}$[s, a]) im TD-Q-Lernen Algorithmus bedeutet, aktualisiere Q-Werte für neue noch unbekannte Zustands/Aktions-Paare Stärker und vertraue den Q-Werten von bereits öfter besuchten Zustand/Aktions-Paaren, sprich je öfter ein Zustand/Aktions-Paar bereits besucht wurde, umso weniger muss der Q-Werte aktualisiert werden.

\item \textbf{$\gamma$} ist der Abschwächungsfaktor (eng. discounting factor). Im fachlichen Umfeld des verstärkenden Lernens wird dieser Abschwächungsfaktor bei Modellen mit unendlichen Horizont verwendet. Endet eine Aktionssequenz in einem Markov-Entscheidungsprozess nicht, dann ist diese unendlich. Um Probleme dieser Klasse trotzdem handhaben zu können, wird für die Berechnung des erwarteten Nutzens $U^\pi(s)$ (siehe \ref{subsec:Optimale Taktiken} Optimale Taktiken Gleichung für den erwarteten Nutzen \ref{eq:Der erwartete Nutzen}) eines Zustands s der Abschwächungsfaktor verwendet. Da sowohl Tic Tac Toe als auch Reversi, nach einer maximalen Anzahl von Aktionen, immer in einem Endzustand terminieren, werden wir den Abschwächungsfaktor gleich 1 setzen. Ein Abschwächungsfaktor von 1 bedeutet, dass Belohnungen in der Zukunft genau so Wertvoll sind wie unmittelbare Belohnungen. 
\end{itemize} 

Konzentrieren wir uns nachfolgend auf die Aktualisierung dieser Q-Werte.

\begin{equation} \label{eq:TD-Q-Aktualisierungsgleichung}
Q(s,a) \leftarrow Q(s,a) + \alpha(R(s) + \gamma \max_{a'} Q(s', a') - Q(s,a))
\end{equation}



\chapter{Realisierung des TD-Q-Agenten (Verstärkendes Lernen)}
\label{cha:Realisierung des TD-Q-Agenten}
''Beim bestärkenden Lernen ist der Lerner ein entscheidungstreffender Agent, der in einer Umgebung Handlungen ausführt und Belohnung (oder Bestrafung) für seine Aktionen beim Versuch, das Problem zu lösen, erfährt. Nach einer Menge an Versuch-und-Irrtum-Durchläufen sollte er die beste Vorgehensweise lernen, welche der Sequenz an Aktionen entspricht, durch welche die Gesamtbelohnung maximiert wird. \cite[397]{Alpaydin}'' \\

''Ein zentrales Problem beim Verbessern der Stellungsbewertung aufgrund von gewonnenen oder verlorenen Partien ist heute bekannt unter dem Namen Credit Assignment. Man hat zwar am Ende des Spiels eine Bewertung des ganzen Spiels, aber keine Bewertung der einzelnen Züge. Der Agent macht also viele Aktionen und erhält erst am Ende ein positives oder negatives Feedback. Wie soll er nun den vielen Aktionen in der Vergangenheit dieses Feedback zuordnen? Und wie soll er dann seine Aktionen gegebenenfalls verbessern? Mit diesen Fragen beschäftigt sich das spannende junge Gebiet des Lernens durch Verstärkung (engl. reinforcement learning) \cite[120]{Ertel}.'' \\

Wir wollen in dieser Arbeit ein Lernverfahren untersuchen und implementieren. Dieses Lernverfahren soll die Strategiespiele Tic Tac Toe und Reversi lernen. Nach den beiden oberen Definitionen, kann ein Agent, der verstärkendes Lernen anwendet, eine annähernd optimalen Strategie, für eine ihm unbekannte Umgebung, lernen. Wir erklären daher in diesem Kapitel verschiedene verstärkende Lernverfahren. Die Wert-Iteration mit der Bellman-Gelichung (siehe Abschnitt \ref{subsec:Wert-Iteration}), die temporale Differenz (siehe Abschnitt \ref{subsec:Lernen mit temporaler Differenz}) und das Q-Lernen (siehe Abschnitt \ref{subsec:Q-Lernen}). \\
\newpage

Das Q-Lernen soll in dieser Arbeit angewendet und untersucht werden. Die Wert-Iteration, ist ein wichtiges Lernverfahren im verstärkenden Lernen. Sie ermöglicht es eine optimale Strategie für Strategiespiele zu lernen. Wert-Iteration hat auch Eigenschaften die im Temporalen Differenz Lernen wiederzufinden sind. Versteht man die Wert-Iteration fällt es leichter das Temporale Differenz Lernen zu verstehen. Das Q-Lernen ist eine Variante des Temporalen Differenz Lernens und wird daher auch als TD-Q-Lernen bezeichnet. \\

Bevor wir jedoch die einzelnen Lernverfahren erläutern, müssen wir noch definieren, wie wir die beiden Strategiespiele Tic Tac Toe und Reversi modellieren werden (siehe Abschnitt \ref{sec:Markov-Entscheidungsprozess}) und wie eigentlich eine optimale Strategie aussehen wird (siehe Abschnitt \ref{sec:Optimale Taktiken}).\\

\section{Markov-Entscheidungsprozess (MEP)}
\label{sec:Markov-Entscheidungsprozess}
Der Markov-Entscheidungsprozess (MEP) oder MDP (engl. Markov decision process) nach Russell und Norvig \cite[752 \psqq]{Russell} ist ein sequentielles Entscheidungsproblem für eine vollständige beobachtbare, stochastische Umgebung mit einem Markov-Übergangsmodell und additiven Gewinnen. Der MEP besteht aus einem Satz von Zuständen (mit einem Anfangszustand $s_0$), einem Satz Actions(s) von Aktionen in jedem Zustand, einem Übergangsmodell P(s'|s, a) und einer Gewinnfunktion R(s). \\

Ganz ähnlich definiert Wolfgang Ertel die Markov-Entscheidungsprozesse \cite[291]{Ertel}. Seine Agenten bzw. die Strategien der Agenten verwenden für die Bestimmung des nächsten Zustandes $s_{t+1}$ nur Informationen über den aktuellen Zustand $s_t$ und nicht über die Vorgeschichte. Dies ist gerechtfertigt, wenn die Belohnung einer Aktion nur von aktuellem Zustand und aktueller Aktion abhängt.\\

(sodass die Lernverfahren auf die Darstellung der Strategiespiele anwendbar sind)

Um darzustellen wie die Strategiespiele exakt auszusehen haben, verwenden wir einen Markov-Entscheidungsprozess, um die Strategiespiele zu modellieren. Die Eigenschaften eines Markov-Entscheidungsprozesses werden auch die Eigenschaften der Strategiespiele. Die Lernverfahren sind nur auf die Eigenschaften des Markov-Entscheidungsprozesses anwendbar. Wir modellieren nachfolgend die Eigenschaften eines MEP auf die beiden Strategiespiele Tic Tac Toe und Reversi. \\

\subsection{Modellierung der Eigenschaften eines MEP auf Tic Tac Toe und Reversi}
Tic Tac Toe und Reversi sind \textbf{sequentielle Entscheidungsprobleme}, denn die einzelnen Züge werden nicht direkt Belohnt, erst am Spielende wird ein Gewinner und ein Verlierer verkündet. Der Agent erhält dafür eine verspätete Verstärkung, die er auf die Spielzugsequenz aufteilen muss (siehe Abschnitt \ref{subsec:Lernen mit temporaler Differenz}).\\

Wolfgang Ertel erklärt vollständig beobachtbare Spiele wie folgt (vlg. \cite[114]{Ertel}):\\
Schach, Reversi, Tic Tac Toe, 4-Gewinnt und Dame sind \textbf{vollständig beobachtbare} Spiele, denn jeder Spieler kennt immer den kompletten Spielzustand. Vollständig beobachtbare Spiele werden auch als Spiele mit vollständiger Information bezeichnet. Viele Kartenspiele wie zum Beispiel Skat, sind nur teilweise beobachtbar, denn der Spieler kennt die Karten des Gegners nicht oder nur teilweise.\\

Aus den Ausführungen von Wolfgang Ertel (vgl. \cite[114]{Ertel}) ist folgende Beschreibung stochastischer Übergänge abzuleiten: \\
Ein \textbf{stochastischer Übergang} ist nur in einer nicht deterministischen Umgebung möglich.  Tic Tac Toe und Reversi sind deterministische Strategiespiele, d.h. jeder Nachfolgezustand ist eindeutig definiert. Eine Aktionssequenz führt also immer zum selben Ergebnis. Es finden keine stochastischen Übergänge in Tic Tac Toe und Reversi statt. Backgammon ist ein nichtdeterministisches Strategiespiel, in diesem werden stochastische Übergänge durch ein Würfelergebnis bestimmt, es ist also vorher nicht eindeutig, welcher Nachfolgezustand durch eine Aktion eintreten wird. \\

Sinngemäße Definition des Übergangsmodells \cite[753]{Russell}: \\
Das \textbf{Übergangsmodell} beschreibt das Ergebnis jeder Aktion in jedem Zustand. Ist das Ergebnis stochastisch, bezeichnet P(s'|s, a) die Wahrscheinlichkeit, den Zustand s' zu erreichen, wenn die Aktion a im Zustand s ausgeführt wird. Handelt es sich um einen Markov-Übergang, dann ist die Wahrscheinlichkeit s' von s zu erreichen, nur von s abhängig und nicht vom Verlauf der vorherigen Zustände. \\

Ein stochastisches Übergangsmodell für die Wahrscheinlichkeiten der Zustandsübergänge ist für Tic Tac Toe und Reversi nicht sinnvoll, da beide Spiele nicht vom Zufall abhängen (keine stochastischen Übergänge) und für jede Aktion in jedem Zustand nur ein einziger Zustandsübergang möglich ist. \newpage

\textbf{Additive Gewinne}, nach Russell und Norvig \cite[756]{Russell}, bestimmen über das zukunftsbezogene Verhalten des Agenten. Verwendet der Agent additive Gewinne, dann bedeutet das für den Agenten, jeder Nutzen eines Zustandes in einer gewählten Zustandsfolge ist gleich wertvoll. Zudem ist die Summe der Zustandsnutzen endlich, deshalb auch Modell des endlichen Horizonts. Der Nutzen einer Zustandsfolge ist wie folgt definiert: \\

$U_h([s_0, s_1, s_2 ...]) = R(s_0) + R(s_1) + R(s_2) + ...$ \\

Spiele die unter Umständen nicht immer einen Endzustand erreichen, haben keinen endlichen Horizont, sondern einen unendlichen Horizont. Für diese Spiele ist ein Modell mit einem endlichen Horizont unangemessen, denn wir wissen nicht, wie lang die Lebensdauer des Agenten ist (vgl. \cite[250]{KLM96}). \\

Tic Tac Toe und Reversi terminieren immer, nach einer endlichen Anzahl von Aktionen, in einem Endzustand. Wir können daher für beide Spiele das Modell des endlichen Horizonts bzw. additive Gewinne verwenden.\\

\textbf{Verminderte Gewinne}, nach Russell und Norvig \cite[756]{Russell}, unterscheiden sich von den additiven Gewinnen durch einen Vermeidungsfaktor $\gamma$. Der Verminderungsfaktor schwächt Zustände in der Zukunft immer weiter ab, d.h. je weiter ein Zustand in der Zukunft liegt, desto mehr wird er abgeschwächt. Der Nutzen für den ersten Zustand der Zustandsfolge wird nicht abgeschwächt. Ist $\gamma$ gleich 1, sind die verminderten Gewinne gleich den additiven Gewinnen. Die additiven Gewinne sind also ein Sonderfall der verminderten Gewinne.\\

$U_h([s_0, s_1, s_2 ...]) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + ...$\\

Wichtiges Fazit daraus: \\
Der Nutzen einer gegebenen Zustandsfolge ist die Summe der verminderten Gewinne, die während der Folge erhalten werden (vgl. \cite[757]{Russell}).
\newpage

\section{Optimale Taktiken}
\label{sec:Optimale Taktiken}
Wir definieren in diesem Abschnitt, nach Russell und Norvig \cite[757\psq]{Russell}, was eine optimale Taktik ist: Eine Taktik (Strategie) beeinflusst das Verhalten des Agenten, d.h. sie empfiehlt welche Aktion der Agent in jedem Zustand ausführen soll. Aus Tradition wird beim verstärkenden Lernen eine Taktik mit dem Symbol $\pi$ gekennzeichnet. Die Abbildung der Zustände auf Aktionen ist folgendermaßen definiert $\pi : S \rightarrow A$ (vgl. \cite[290]{Ertel}) oder $\pi(s) = a$. Abhängig von den Dimensionen der Umgebung existieren unterschiedlich viele Taktiken. Eine optimale Taktik wird bestimmt durch den erwarteten Nutzen bei Ausführung der Taktik $\pi$ beginnend in einem Startzustand s:\\

\begin{equation}
\label{eq:Der erwartete Nutzen}
U^\pi(s) = E\left[\sum_{t=0}^{\infty} \gamma^t R(S_t)\right].
\end{equation}

Eine optimale Taktik hat im Vergleich zu allen anderen möglichen Taktiken einen gleich hohen oder höheren erwarteten Nutzen. Eine solche optimale Taktik wird gekennzeichnet durch $\pi^*_s$: \\ 

\begin{equation}
\pi^*_s = \argmax_\pi U^\pi(s).
\end{equation}

Es ist möglich, dass mehrere optimale Taktiken für ein Problem existieren. Russell und Norvig erklären, dass für eine optimale Strategie $\pi^*_s$, auch $\pi^*$ geschrieben werden kann, denn wenn Taktik $\pi^*_a$ optimal beim Beginn in a und Taktik $\pi^*_b$ optimal beim Start in b sind und sie einen dritten Zustand c erreichen, gibt es keinen vernünftigen Grund, dass sie untereinander oder mit $\pi^*_c$ nicht übereinkommen. \\

Mit diesen Definitionen ist der wahre Nutzen eines Zustands einfach $U^{\pi^*}(s) -$ d.h. die erwartete Summe verminderter Gewinne, wenn der Agent eine optimale Taktik ausführt. Wir schreiben dies als U(s). Russell und Norvig unterstreichen den Sachverhalt, dass die Funktionen U(s) und R(s) ganz unterschiedliche Quantitäten sind, denn R(s) gibt den ''kurzfristigen'' Gewinn, sich in s zu befinden an, wohingegen U(s) den ''langfristigen'' Gesamtgewinn ab s angibt.
\newpage

\section{Verstärkende Lernverfahren}
Verstärkendes Lernen (eng. reinforcement Learning) ist eine Lernkategorie des maschinellen Lernens. Problemstellungen des verstärkenden Lernens sind, u.a. das lernen von Strategiespielen, wie Schach, Reversi, Dame oder Backgammon. Der theoretische verstärkend lernende Lösungsansatz dieser Probleme ist wie folgt: ein Agent soll ein ihm unbekanntes Strategiespiel lernen (das Strategiespiel ist die unbekannte Umgebung), für einen Spielzug  (Aktion) in einer Spielsituation (Zustand) erhält der Agent eine numerische Belohnung oder Bestrafung (Verstärkung), mittels dieser Verstärkung soll der Agent ein optimales Verhalten in der ihm unbekannte Umgebung erlernen. \\

Es gibt im verstärkenden Lernen mehrere Lernverfahren, die wir nachfolgend erläutern (Abschnitt \ref{subsec:Wert-Iteration} bis Abschnitt \ref{subsec:Q-Lernen}). \\

\subsection{Dynamische Programmierung und Wert-Iteration}
\label{subsec:Wert-Iteration}
Verwenden wir bereits vorhandenes Wissen über Strategien und speichern dieses in Zwischenergebnisse über Teile von Strategien, dann bezeichnen wir diese Vorgehensweise zur Lösung von Optimierungsproblemen als dynamische Programmierung. Diese Vorgehensweise wurde bereits 1957 von Richard Bellman beschrieben \cite[293]{Ertel}. \\

Im Abschnitt optimale Taktiken haben wir gezeigt, dass der Nutzen U, in einem Zustand s, unter Beachtung einer Strategie $\pi$, berechnet werden kann, aus der Summe aller abgeschwächten Belohnungen, für jeden besuchten Zustand, in einem Zeitintervall von $t = 0$ bis $\infty$. Dementsprechend gibt eine optimale Taktik $\pi^*(s)$ für jeden Zustand s den Nachfolgezustand mit dem größtmöglichen erwarteten Nutzen an (siehe \cite[759]{Russell}):

\begin{equation}
\pi^*(s) = \argmax_{a \in A(s)} \sum_{s'} P(s'|s, a) U(s').
\end{equation}

''Daraus folgt, dass es eine direkte Beziehung zwischen dem Nutzen eines Zustandes und dem Nutzen seiner Nachbarn gibt. Der Nutzen eines Zustandes ist der unmittelbare Gewinn für diesen Zustand plus dem erwarteten verminderten Gewinn des nächsten Zustandes, vorausgesetzt, der Agent wählt die optimale Aktion. Das bedeutet, der Nutzen eines Zustandes ist gegeben durch:

\begin{equation}
U(s) = R(s) + \gamma \max_{a \in A(s)} \sum_{s'} P(s'|s, a) U(s').
\end{equation}

Diese Gleichung wird als Bellman-Gleichung bezeichnet, nach Richard Bellman(1957). Die Nutzen der Zustände - durch Gleichung \ref{eq:Der erwartete Nutzen} als die erwarteten Nutzen nachfolgender Zustandsfolgen definiert - sind Lösungen der Menge der Bellman-Gleichungen. \cite[759]{Russell}''\\

Die aus der Bellman-Gleichung formulierbare rekursive Aktualisierungsregel, auch die Bellman-Aktualisierung genannt, ist Hauptbestandteil des Wert-Iteration Algorithmus. Wolfgang Ertel notiert diese Aktualisierungsregel wie folgt \cite[294]{Ertel}:

\begin{equation}
\hat{V}(s) = \max_a [r(s,a) + \gamma \hat{V}(\delta(s,a))].
\end{equation}

Dahingegen notieren Russell und Norvig die Bellman-Aktualisierung etwas anders \cite[760]{Russell}:

\begin{equation}
U_{i+1}(s) \leftarrow R(s) + \gamma \max_{a \in A(s)} \sum_{s'} P(s'|s,a)U_i(s').
\end{equation} 

Betrachten wir jetzt die Äquivalenzen der beiden Gleichungen. Ertel bezeichnet den Iterationsschritt in einem Zustand s als $\hat{V}(s)$ und Russell und Norvig definieren den Nutzwert für den Zustand s bei der i-ten Iteration als $U_{i}(s)$ und den Iterationsschritt bezeichnen sie als $U_{i+1}$. Die Gewinnfunktionen R(s) und r(s,a) sind leicht unterschiedlich. Funktion R(s) gibt den direkten Gewinn in einem Zustand s an und Funktion r(s,a) den Gewinn für eine Aktion, die im Zustand s ausgeführt wird. Die Funktionen $\max_a$ und $\max_{a \in A(s)}$ berechnen die Aktion a mit dem höchsten erwarteten Nutzen. Das stochastische Modell der Welt wird durch die Funktionen $\delta(s,a)$ und $P(s'|s,a)$ beschrieben. Beide Funktionen bilden die Wahrscheinlichkeit ab, dass ein Zustand s' erreicht wird, wenn eine Aktion a in Zustand s ausgeführt wird. \\

Den wahren Nutzen haben wir definiert als die erwartete Summe verminderter Gewinne. Die Verminderung wird in beiden Gleichungen durch den Abschwächungsfaktor $\gamma$ notiert. Die erwartete Summe verminderter Gewinne ist die Summe aller Iterationsschritte bis zur Konvergenz beider Gleichungen. Der rekursive Funktionsaufruf in der Aktualisierungsregel von Wolfgang Ertel $\hat{V}(\delta(s,a))$ übergibt dem nächsten Iterationsschritt den Zustand s', der zu einer von $\delta$ bzw. von der Umgebung festgelegten Wahrscheinlichkeit eintrifft. In der Aktualisierungsgleichung von Russell und Norvig wird dies durch die Kombination des stochastischen Modells P(s'|s,a) und dem rekursiven Funktionsaufruf $U_i(s')$ realisiert.
\newpage

Ein Lernverfahren (z.B. die adaptive dynamische Programmierung), welches die Wert-Iteration nutzt, wird im Rahmen dieser Arbeit jedoch nicht implementiert. Die dynamische Programmierung und die Wert-Iteration sind sehr wichtige Ansätze für Lernverfahren und die nachfolgenden Lernverfahren sind teilweise sehr eng mit Lernverfahren verwandt, die Wert-Iteration verwenden. \\

\subsection{Lernen mit temporaler Differenz (TD-Lernen)}
\label{subsec:Lernen mit temporaler Differenz}
Bei dieser Lernmethode werden die Nutzen der beobachteten Zustände an die beobachteten Übergänge angepasst, sodass sie mit den Bedingungsgleichungen (siehe Bellman-Gleichung) übereinstimmen. \\

''Allgemeiner können wir sagen, wenn ein Übergang vom Zustand s in den Zustand s' stattfindet, wenden wir die folgende Aktualisierung mit $U^\pi(s)$ an:\\
\begin{equation}
\label{eq:Aktualisierungsgleichung temporale Differenz}
U^\pi(s) \leftarrow U^\pi(s) + \alpha(R(s) + \gamma U^\pi(s') - U^\pi(s)).
\end{equation}

Hier ist $\alpha$ der Lernratenparameter. Weil diese Aktualisierungsregel die Differenz der Nutzen aufeinanderfolgender Zustände verwendet, wird sie auch häufig als TD-Gleichung (Temporale Differenz) bezeichnet. \cite[966\psq]{Russell}'' \\ 

Der Lernratenparameter $\alpha$ gibt an, wie stark neue Nutzwerte die derzeitige Bewertungsfunktion anpassen können. 

\subsection{Q-Lernen (TD-Q-Lernen)}
\label{subsec:Q-Lernen}
Wir beschreiben das Q-Lernen anhand der Ausführungen von Russell und Norvig (vgl. \cite[973 \psq]{Russell}): \\

Das Q-Lernen ist eine Variante des TD-Lernens und wird auch als TD-Q-Lernen bezeichnet. Die Aufgabe des TD-Q-Lernens ist eine optimale Strategie zu entwickeln. Das TD-Q-Lernen lernt nicht, wie bei einer Wert-Iteration, eine wahre Nutzenfunktion U(s), sondern eine Q-Funktion. Eine Q-Funktion ist eine Abbildung von Zustands/Aktions-Paaren auf Nutzwerte. Q-Werte sind wie folgt mit Nutzwerten verknüpf \cite[974]{Russell}:

\begin{equation}
\label{eq:Nutzenwerte und Q-Werte}
U(s) = \max_a Q(s,a).
\end{equation}

\newpage

''Ein TD-Q-Agent der eine Q-Funktion lernt, braucht weder für das Lernen noch die Aktionsauswahl ein Modell der Form P(s'|s,a). Aus diesem Grund sagt man auch, das Q-Lernen ist eine modellfreie Methode. \cite[974]{Russell}'' \\

Dahingegen ist eine Nutzenfunktion U(s) abhängig von den abgeschwächten Nutzwerten aller nachfolgenden Zustände. \\

Die Aktualisierungsgleichung für TD-Q-Lernen lautet \cite[974]{Russell}: \\

\begin{equation}
Q(s,a) \leftarrow Q(s,a) + \alpha(R(s) + \gamma \max_{a'} Q(s',a') - Q(s,a)).
\end{equation}

Diese Aktualisierungsgleichung für TD-Q-Lernen wird Kernstück unserer Implementierung für den TD-Q-Agenten. \\

Was ist jedoch der Unterschied zwischen einer Belohnungsfunktion r(s, a) und einer Q-Funktion Q(s, a)? Die Funktion r(s, a) ist von der Umgebung definiert und kann vom Agenten nicht beeinflusst werden. Sollte diese Funktion dem Agenten eine numerische Verstärkung von -0,5 zuweisen, dann kann der Agent dies nicht ändern. Der Agent soll versuchen die Zusammenhänge der Zustands/Aktions-Paare zu lernen und Entscheidungen basierend auf seinen Lernerfahrungen zu treffen. Dies bezeichnen wir dann als Q-Lernen. Die vom Agenten gelernten zusammenhänge werden in Q-Werten gespeichert. Folglich wird in Q(s, a) oder Q[s, a] die gelernte Erfahrung des Agenten, für ein Zustand/Aktions-Paar, gespeichert. \\

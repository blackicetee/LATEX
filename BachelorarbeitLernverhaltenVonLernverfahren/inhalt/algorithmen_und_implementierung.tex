\chapter{Algorithmen und Implementierung}
\label{cha:Algorithmen und Implementierung}

In diesem Kapitel: //TODO Einführung in das Kapitel

\section{Tic Tac Toe}

\section{Reversi}

\section{Suchbaumverfahren}

\section{Heuristiken}

\section{TD-Q-Lernen}
In dem Kapitel \ref{cha:Einführung in verstärkendes Lernen} Einführung in verstärkendes Lernen, speziell in den Abschnitte \ref{sec:Temporale Differenz Lernen} Temporale Differenz Lernen und \ref{sec:Q-Lernen} Q-Lernen wurde bereits erklärt, wie das TD-Lernen und das Q-Lernen zusammenwirken. \\

Fassen wir diese beiden Abschnitte kurz zusammen: Temporale Differenz Lernen (TD-Lernen) passt die Nutzen der beobachteten Zustände an die beobachteten Übergänge an. Die Aktualisierungsregel (Gleichung \ref{eq:Aktualisierungsgleichung temporale Differenz}) des TD-Lernens verwendet die Differenz der Nutzen aufeinanderfolgender Zustände $U\pi(s') - U\pi(s)$, daher die Bezeichnung Temporale Differenz Lernen. Eine alternative TD-Methode ist das Q-Lernen, dass statt Nutzen eine Aktion/Nutzen Repräsentation lernt. Mit der Notation Q(s,a) bezeichnen wir den Wert der Ausführung von Aktion a im Zustand s. Gleichung \ref{eq:Nutzenwerte und Q-Werte} zeigt wie Q-Werte direkt mit Nutzenwerten verknüpft sind. \\

Der in Abbildung \ref{fig:TD-Q-Lernen Algorithmus} skizzierte Algorithmus ist, leicht abgewandelt, im Lehrbuch für künstliche Intelligenz von Russell und Norvig \cite[974]{Russell} zu finden. Dieser ist bereits leicht modifiziert in seiner Notation und nachdem wir die Einzelheiten des Algorithmus geklärt haben, wird die Notation weiter angepasst, bis sie dem Quellcode des Prototypen entspricht. \\

\begin{figure}[!htbp]
\centering
\begin{lstlisting}[frame=single, mathescape=true]
1  def Q-Lernen(s', r', $\alpha$, $\gamma$):
2    if istTerminalzustand(s):
3      Q[s, None] $\leftarrow$ r'
4    if s ist nicht None:
5      inkrementiere $N_{sa}$[s, a]
6      Q[s, a] $\leftarrow$ Q[s, a] + $\alpha$(N$_{sa}${s, a])
  		* (r + $\gamma max_{a'}$ Q[s', a'] - Q[s, a])
7    s, a, r $\leftarrow$ s', argmax$_{a'}$, f(Q[s', a'], N$_{sa}$), r'
8    return a
\end{lstlisting}
\caption{TD-Q-Lernen Algorithmus}
\label{fig:TD-Q-Lernen Algorithmus}
\end{figure} 

Der Q-Lernen Algorithmus verwendet einige persistente (d.h. beständige oder dauerhafte) Variablen und Entitäten. Persistent deshalb, weil sie die einzelnen Funktionsaufrufe überdauern: 
\begin{itemize}

\item \textbf{Q} ist eine Tabelle mit Aktionswerden, indiziert nach Zustand und Aktion. Der Aufruf Q[s, a] liefert z.B. einen Aktionswert (Q-Wert) für eine Aktion a in einem Zustand s. Zu beginn des Lernprozesses sind alle Werte dieser Tabelle leer.

\item \textbf{N$_{sa}$} ist eine Tabelle mit Häufigkeiten für Zustand/Aktions-Paare.  Diese ist wie Q anfangs leer. Jedes mal wenn ein Zustand/Aktions-Paar durchlaufen wird, welches bereits durchlaufen wurde, dann wird der Tabelleneintrag N$_{sa}$[s, a] inkrementiert d.h. um den Wert 1 erhöht.

\item \textbf{s} ist der vorhergehende Spielzustand, anfangs leer. Berücksichtigen wir den Zeitlichen Aspekt, dann wäre s zu einem Zeitpunkt t geschrieben s${_t}$ und ein darauffolgender Spielzustand wäre s${_t+1}$. Der direkt auf s folgende Spielzustand wird auch als s' (s Prime) bezeichnet.

\item \textbf{a} ist die vorhergehende Aktion, anfangs leer. Wird die Aktion a im Zustand s ausgeführt, dann wird der Zustand s' bzw. s$_{t+1}$ erreicht. Eine Aktion die in s' ausgeführt werden kann bezeichnen wir als a' oder a$_{t+1}$.

\item \textbf{r} ist die Belohnung die dem Agenten von der Umgebung zugeteilt wird, anfangs leer, wenn der Agent eine Aktion a in einem Zustand s ausführt. Wir können eine Funktion r(s, a) definieren. Die Funktion r(s, a) wird für die meisten Spielzustände s $\in$ S den Wert 0 liefern. Für Endzustände der jeweiligen Strategiespiele wird die Funktion r(s, a) andere Werte liefern. Ist r die Belohnung dafür Aktion a in Zustand s auszuführen, dann ist r' (r Prime) die Belohnung dafür Nachfolgeaktion a' in Nachfolgezustand s' auszuführen.
\end{itemize}

Der Q-Lernen Algorithmus bekommt folgende Eingabeparameter übergeben:

\begin{itemize}
\item \textbf{s'} ist der aktuelle Spielzustand und gleichzusetzen mit der aktuellen Wahrnehmung des Agenten. Wie bereits erklärt ist s' der Nachfolgezustand von s.

\item \textbf{r'} ist das Belohnungssignal, welches der Agent erhält, wenn er eine Aktion a' im Zustand s' ausführt. 

\item $\boldsymbol{\alpha}$ ist bestimmt über die Lernrate des Algorithmus. Der Wert von $\alpha$ ist in der Regel zwischen 0 und 1. Eine hohe Lernrate ($\alpha$ nahe 1) bedeutet, dass die Aktualisierung des Q-Werts stärker ist. Bei einer niedrigen Lernrate ist die Aktualisierung schwächer. Der Ausdruck $\alpha$(N$_{sa}$[s, a]) im TD-Q-Lernen Algorithmus bedeutet, aktualisiere Q-Werte für neue noch unbekannte Zustands/ Aktions-Paare Stärker und vertraue den Q-Werten von bereits öfter besuchten Zustand/ Aktions-Paaren, sprich je öfter ein Zustand/Aktions-Paar bereits besucht wurde, umso weniger muss der Q-Werte aktualisiert werden.

\item $\boldsymbol{\gamma}$ ist der Abschwächungsfaktor (eng. discounting factor). Im fachlichen Umfeld des verstärkenden Lernens wird dieser Abschwächungsfaktor bei Modellen mit unendlichen Horizont verwendet. Endet eine Aktionssequenz in einem Markov-Entscheidungsprozess nicht, dann ist diese unendlich. Um Probleme dieser Klasse trotzdem handhaben zu können, wird für die Berechnung des erwarteten Nutzens $U^\pi(s)$ (siehe \ref{subsec:Optimale Taktiken} Optimale Taktiken Gleichung für den erwarteten Nutzen \ref{eq:Der erwartete Nutzen}) eines Zustands s der Abschwächungsfaktor verwendet. Da sowohl Tic Tac Toe als auch Reversi, nach einer maximalen Anzahl von Aktionen, immer in einem Endzustand terminieren, werden wir den Abschwächungsfaktor gleich 1 setzen. Ein Abschwächungsfaktor von 1 bedeutet, dass Belohnungen in der Zukunft genau so Wertvoll sind wie unmittelbare Belohnungen. 
\end{itemize} 

Konzentrieren wir uns nachfolgend auf die Aktualisierung dieser Q-Werte.

\begin{equation} \label{eq:TD-Q-Aktualisierungsgleichung}
Q(s,a) \leftarrow Q(s,a) + \alpha(R(s) + \gamma \max_{a'} Q(s', a') - Q(s,a))
\end{equation}